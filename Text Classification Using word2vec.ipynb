{
 "cells": [
  {
   "source": [
    "# Can NLP help to distinguish the authors of two books?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Here we will use Jane Austen's *Persuasion* and Lewis Carroll's *Alice's Adventures in Wonderland* from NLTK's Gutenberg module. \n",
    "### The unit of observation (*documents*) will be the sentences of these novels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import spacy\n",
    "import re\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "import warnings\n",
    "from sklearn import (datasets, model_selection, feature_extraction, linear_model, naive_bayes, ensemble)\n",
    "import collections\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "import multiprocessing as mp \n",
    "import textacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "#!python -m spacy download en\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('gutenberg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### A helper function for removing some punctuation marks and numbers from the text:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# DO NOT DELETE ############################################# \n",
    "# Function to move specific column to the left side for easier view\n",
    "def move_to_left(df, column_name):\n",
    "    df= df[ [str(column_name)] + [ col for col in df.columns if col != str(column_name) ] ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the data\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cleaned texts are stored in two variables called `alice` and `persuasion`. Later, we will split the texts into sentences. We will use spaCy English module and use spaCy to parse both the `alice` and `persuasion` texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take some time.\n",
    "#nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text   author\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                                      (Oh, dear, !)  Carroll"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(So, she, was, considering, in, her, own, mind...</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(Oh, dear, !)</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(Oh, dear, !)</td>\n      <td>Carroll</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# POSs\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one DataFrame\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
    "sentences.head()"
   ]
  },
  {
   "source": [
    "## Tokenize and Lemmaitize "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "\n",
    "for doc in nlp.pipe(sentences['text'].astype('unicode').values, batch_size=50,\n",
    "                        n_threads=3):\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc])\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "        \n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)\n",
    "\n",
    "sentences['tokens'] = tokens\n",
    "sentences['lemma'] = lemma\n",
    "sentences['pos'] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text   author  \\\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll   \n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll   \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Alice, was, beginning, to, get, very, tired, ...   \n",
       "1  [So, she, was, considering, in, her, own, mind...   \n",
       "2  [There, was, nothing, so, VERY, remarkable, in...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  [Alice, be, begin, to, get, very, tired, of, s...   \n",
       "1  [so, -PRON-, be, consider, in, -PRON-, own, mi...   \n",
       "2  [there, be, nothing, so, very, remarkable, in,...   \n",
       "\n",
       "                                                 pos  \n",
       "0  [PROPN, AUX, VERB, PART, AUX, ADV, ADJ, ADP, V...  \n",
       "1  [ADV, PRON, AUX, VERB, ADP, DET, ADJ, NOUN, PU...  \n",
       "2  [PRON, AUX, PRON, ADV, ADV, ADJ, ADP, DET, PUN...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>author</th>\n      <th>tokens</th>\n      <th>lemma</th>\n      <th>pos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n      <td>Carroll</td>\n      <td>[Alice, was, beginning, to, get, very, tired, ...</td>\n      <td>[Alice, be, begin, to, get, very, tired, of, s...</td>\n      <td>[PROPN, AUX, VERB, PART, AUX, ADV, ADJ, ADP, V...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(So, she, was, considering, in, her, own, mind...</td>\n      <td>Carroll</td>\n      <td>[So, she, was, considering, in, her, own, mind...</td>\n      <td>[so, -PRON-, be, consider, in, -PRON-, own, mi...</td>\n      <td>[ADV, PRON, AUX, VERB, ADP, DET, ADJ, NOUN, PU...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n      <td>Carroll</td>\n      <td>[There, was, nothing, so, VERY, remarkable, in...</td>\n      <td>[there, be, nothing, so, very, remarkable, in,...</td>\n      <td>[PRON, AUX, PRON, ADV, ADV, ADJ, ADP, DET, PUN...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "sentences.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['text', 'author', 'tokens', 'lemma', 'pos'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "sentences.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text   author  \\\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll   \n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Alice, was, beginning, to, get, very, tired, ...   \n",
       "1  [So, she, was, considering, in, her, own, mind...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  [Alice, be, begin, to, get, very, tired, of, s...   \n",
       "1  [so, -PRON-, be, consider, in, -PRON-, own, mi...   \n",
       "\n",
       "                                                 pos  \n",
       "0  [PROPN, AUX, VERB, PART, AUX, ADV, ADJ, ADP, V...  \n",
       "1  [ADV, PRON, AUX, VERB, ADP, DET, ADJ, NOUN, PU...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>author</th>\n      <th>tokens</th>\n      <th>lemma</th>\n      <th>pos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n      <td>Carroll</td>\n      <td>[Alice, was, beginning, to, get, very, tired, ...</td>\n      <td>[Alice, be, begin, to, get, very, tired, of, s...</td>\n      <td>[PROPN, AUX, VERB, PART, AUX, ADV, ADJ, ADP, V...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(So, she, was, considering, in, her, own, mind...</td>\n      <td>Carroll</td>\n      <td>[So, she, was, considering, in, her, own, mind...</td>\n      <td>[so, -PRON-, be, consider, in, -PRON-, own, mi...</td>\n      <td>[ADV, PRON, AUX, VERB, ADP, DET, ADJ, NOUN, PU...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "sentences.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas list to string\n",
    "sentences['lemma'] = [', '.join(map(str, l)) for l in sentences['lemma']]\n",
    "sentences['pos'] = [', '.join(map(str, l)) for l in sentences['pos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word2vec on the sentences\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences[\"lemma\"],\n",
    "    workers=4,\n",
    "    min_count=1,\n",
    "    window=6,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=100,\n",
    "    hs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"word 'lady' not in vocabulary\"",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d798328b0568>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lady'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'man'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'woman'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoesnt_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dad dinner mom aunt uncle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'woman'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'man'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'horse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1459\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m                 )\n\u001b[1;32m-> 1461\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m         \"\"\"\n\u001b[1;32m-> 1383\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'lady' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['lady', 'man'], negative=['woman'], topn=5))\n",
    "print(model.doesnt_match(\"dad dinner mom aunt uncle\".split()))\n",
    "print(model.similarity('woman', 'man'))\n",
    "print(model.similarity('horse', 'cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize Lemmas\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "X = vectorizer.fit_transform(sentences['lemma'])\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "df_lemma = pd.concat([bow_df, sentences[[ \"author\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5632, 5094)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "df_lemma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize Lemmas + POS\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "X = vectorizer.fit_transform(sentences['pos'])\n",
    "bow_pos = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "df_lemma_pos = pd.concat([bow_pos, df_lemma], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5632, 5109)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df_lemma_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize 2-grams\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(sentences['lemma'])\n",
    "bow_ngram = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "df_lemma_pos_2gram = pd.concat([bow_ngram, df_lemma_pos], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5632, 44899)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df_lemma_pos_2gram.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Use multiple machine learning algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemma_s = df_lemma.sample(5000)\n",
    "df_lemma_pos_s = df_lemma_pos.sample(5000)\n",
    "df_lemma_pos_2gram_s = df_lemma_pos_2gram.sample(5000)"
   ]
  },
  {
   "source": [
    "## Use of Machine Learning on Lemmas only"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_lemma_s['author']\n",
    "X = np.array(df_lemma_s.drop(['author'], 1))\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "lr_params = {\"penalty\": [\"l2\"]}\n",
    "lr = LogisticRegression()\n",
    "\n",
    "rfc_params = {\"n_estimators\": [3, 5, 10, 15], \"max_depth\": [2, 3, 4, 5], \"min_samples_split\": [3, 5, 7, 9]}\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "gbc_params = {\"n_estimators\": [3, 5, 10, 15],\"max_depth\": [2, 3, 4, 5], \"min_samples_split\": [3, 5, 7, 9]}\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "SDG_params= {'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e3], 'penalty': ['l2'], 'n_jobs': [-1]}\n",
    "SDG = linear_model.SGDClassifier()\n",
    "\n",
    "NB_Multi_params = {'alpha': [1.0]}\n",
    "NB_Multi= naive_bayes.MultinomialNB()\n",
    "\n",
    "NB_Bern_params = {'alpha': [1.0]}\n",
    "NB_Bern = naive_bayes.BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=BernoulliNB(), param_grid={'alpha': [1.0]})"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# GridsearchCV\n",
    "clf_lr = GridSearchCV(lr, lr_params, cv=5)\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "clf_rfc = GridSearchCV(rfc, rfc_params, cv=5)\n",
    "clf_rfc.fit(X_train, y_train)\n",
    "\n",
    "clf_gbc = GridSearchCV(gbc, gbc_params, cv=5)\n",
    "clf_gbc.fit(X_train, y_train)\n",
    "\n",
    "clf_SDG = GridSearchCV(SDG, SDG_params, cv=5)\n",
    "clf_SDG.fit(X_train, y_train)\n",
    "\n",
    "clf_NB_Multi = GridSearchCV(NB_Multi, NB_Multi_params, cv=5)\n",
    "clf_NB_Multi.fit(X_train, y_train)\n",
    "\n",
    "clf_NB_Bern = GridSearchCV(NB_Bern, NB_Bern_params, cv=5)\n",
    "clf_NB_Bern.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# Fit the models\n",
    "lr.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "SDG.fit(X_train, y_train)\n",
    "NB_Multi.fit(X_train, y_train)\n",
    "NB_Bern.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.9523333333333334\n",
      "\n",
      "Test set score: 0.8825\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.9946666666666667\n",
      "\n",
      "Test set score: 0.8525\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.8553333333333333\n",
      "\n",
      "Test set score: 0.834\n",
      "----------------------Stochastic Gradient Descent Scores----------------------\n",
      "Training set score: 0.98\n",
      "\n",
      "Test set score: 0.8745\n",
      "---------------------- Naive Bayes Multinominal Scores----------------------\n",
      "Training set score: 0.9336666666666666\n",
      "\n",
      "Test set score: 0.911\n",
      "----------------------Naive Bayes Bernoulli Scores----------------------\n",
      "Training set score: 0.8923333333333333\n",
      "\n",
      "Test set score: 0.8715\n"
     ]
    }
   ],
   "source": [
    "# Check some basic performance\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Random Forest Scores----------------------\")\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "print('Training set score:', gbc.score(X_train, y_train))\n",
    "print('\\nTest set score:', gbc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Stochastic Gradient Descent Scores----------------------\")\n",
    "print('Training set score:', SDG.score(X_train, y_train))\n",
    "print('\\nTest set score:', SDG.score(X_test, y_test))\n",
    "\n",
    "print(\"---------------------- Naive Bayes Multinominal Scores----------------------\")\n",
    "print('Training set score:', NB_Multi.score(X_train, y_train))\n",
    "print('\\nTest set score:', NB_Multi.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Naive Bayes Bernoulli Scores----------------------\")\n",
    "print('Training set score:', NB_Bern.score(X_train, y_train))\n",
    "print('\\nTest set score:', NB_Bern.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Use of Machine Learning on Lemmas + POS only"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_lemma_pos_s['author']\n",
    "X = np.array(df_lemma_pos_s.drop(['author'], 1))\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "lr_params = {\"penalty\": [\"l2\"]}\n",
    "lr = LogisticRegression()\n",
    "\n",
    "rfc_params = {\"n_estimators\": [3, 5, 10, 15], \"max_depth\": [2, 3, 4, 5], \"min_samples_split\": [3, 5, 7, 9]}\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "gbc_params = {\"n_estimators\": [3, 5, 10, 15],\"max_depth\": [2, 3, 4, 5], \"min_samples_split\": [3, 5, 7, 9]}\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "SDG_params= {'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e3], 'penalty': ['l2'], 'n_jobs': [-1]}\n",
    "SDG = linear_model.SGDClassifier()\n",
    "\n",
    "NB_Multi_params = {'alpha': [1.0]}\n",
    "NB_Multi= naive_bayes.MultinomialNB()\n",
    "\n",
    "NB_Bern_params = {'alpha': [1.0]}\n",
    "NB_Bern = naive_bayes.BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=BernoulliNB(), param_grid={'alpha': [1.0]})"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# GridsearchCV\n",
    "clf_lr = GridSearchCV(lr, lr_params, cv=5)\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "clf_rfc = GridSearchCV(rfc, rfc_params, cv=5)\n",
    "clf_rfc.fit(X_train, y_train)\n",
    "\n",
    "clf_gbc = GridSearchCV(gbc, gbc_params, cv=5)\n",
    "clf_gbc.fit(X_train, y_train)\n",
    "\n",
    "clf_SDG = GridSearchCV(SDG, SDG_params, cv=5)\n",
    "clf_SDG.fit(X_train, y_train)\n",
    "\n",
    "clf_NB_Multi = GridSearchCV(NB_Multi, NB_Multi_params, cv=5)\n",
    "clf_NB_Multi.fit(X_train, y_train)\n",
    "\n",
    "clf_NB_Bern = GridSearchCV(NB_Bern, NB_Bern_params, cv=5)\n",
    "clf_NB_Bern.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# Fit the models\n",
    "lr.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "SDG.fit(X_train, y_train)\n",
    "NB_Multi.fit(X_train, y_train)\n",
    "NB_Bern.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.954\n",
      "\n",
      "Test set score: 0.8775\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.9936666666666667\n",
      "\n",
      "Test set score: 0.8445\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.8776666666666667\n",
      "\n",
      "Test set score: 0.837\n",
      "----------------------Stochastic Gradient Descent Scores----------------------\n",
      "Training set score: 0.976\n",
      "\n",
      "Test set score: 0.882\n",
      "---------------------- Naive Bayes Multinominal Scores----------------------\n",
      "Training set score: 0.925\n",
      "\n",
      "Test set score: 0.893\n",
      "----------------------Naive Bayes Bernoulli Scores----------------------\n",
      "Training set score: 0.854\n",
      "\n",
      "Test set score: 0.8365\n"
     ]
    }
   ],
   "source": [
    "# Check some basic performance\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Random Forest Scores----------------------\")\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "print('Training set score:', gbc.score(X_train, y_train))\n",
    "print('\\nTest set score:', gbc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Stochastic Gradient Descent Scores----------------------\")\n",
    "print('Training set score:', SDG.score(X_train, y_train))\n",
    "print('\\nTest set score:', SDG.score(X_test, y_test))\n",
    "\n",
    "print(\"---------------------- Naive Bayes Multinominal Scores----------------------\")\n",
    "print('Training set score:', NB_Multi.score(X_train, y_train))\n",
    "print('\\nTest set score:', NB_Multi.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Naive Bayes Bernoulli Scores----------------------\")\n",
    "print('Training set score:', NB_Bern.score(X_train, y_train))\n",
    "print('\\nTest set score:', NB_Bern.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Use of Machine Learning on Lemmas + POS + 2grams only"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_lemma_pos_2gram_s['author']\n",
    "X = np.array(df_lemma_pos_2gram_s.drop(['author'], 1))\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "lr_params = {\"penalty\": [\"l2\"]}\n",
    "lr = LogisticRegression()\n",
    "\n",
    "rfc_params = {\"n_estimators\": [3, 5, 10, 15], \"max_depth\": [2, 3, 4, 5], \"min_samples_split\": [3, 5, 7, 9]}\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "gbc_params = {\"n_estimators\": [3, 5, 10, 15],\"max_depth\": [2, 3, 4, 5], \"min_samples_split\": [3, 5, 7, 9]}\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "SDG_params= {'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e3], 'penalty': ['l2'], 'n_jobs': [-1]}\n",
    "SDG = linear_model.SGDClassifier()\n",
    "\n",
    "NB_Multi_params = {'alpha': [1.0]}\n",
    "NB_Multi= naive_bayes.MultinomialNB()\n",
    "\n",
    "NB_Bern_params = {'alpha': [1.0]}\n",
    "NB_Bern = naive_bayes.BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=BernoulliNB(), param_grid={'alpha': [1.0]})"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# GridsearchCV\n",
    "clf_lr = GridSearchCV(lr, lr_params, cv=5)\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "clf_rfc = GridSearchCV(rfc, rfc_params, cv=5)\n",
    "clf_rfc.fit(X_train, y_train)\n",
    "\n",
    "clf_gbc = GridSearchCV(gbc, gbc_params, cv=5)\n",
    "clf_gbc.fit(X_train, y_train)\n",
    "\n",
    "clf_SDG = GridSearchCV(SDG, SDG_params, cv=5)\n",
    "clf_SDG.fit(X_train, y_train)\n",
    "\n",
    "clf_NB_Multi = GridSearchCV(NB_Multi, NB_Multi_params, cv=5)\n",
    "clf_NB_Multi.fit(X_train, y_train)\n",
    "\n",
    "clf_NB_Bern = GridSearchCV(NB_Bern, NB_Bern_params, cv=5)\n",
    "clf_NB_Bern.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "# Fit the models\n",
    "lr.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "SDG.fit(X_train, y_train)\n",
    "NB_Multi.fit(X_train, y_train)\n",
    "NB_Bern.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.9836666666666667\n",
      "\n",
      "Test set score: 0.886\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.9933333333333333\n",
      "\n",
      "Test set score: 0.841\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.8823333333333333\n",
      "\n",
      "Test set score: 0.847\n",
      "----------------------Stochastic Gradient Descent Scores----------------------\n",
      "Training set score: 0.9863333333333333\n",
      "\n",
      "Test set score: 0.8845\n",
      "---------------------- Naive Bayes Multinominal Scores----------------------\n",
      "Training set score: 0.937\n",
      "\n",
      "Test set score: 0.883\n",
      "----------------------Naive Bayes Bernoulli Scores----------------------\n",
      "Training set score: 0.8686666666666667\n",
      "\n",
      "Test set score: 0.8385\n"
     ]
    }
   ],
   "source": [
    "# Check some basic performance\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Random Forest Scores----------------------\")\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "print('Training set score:', gbc.score(X_train, y_train))\n",
    "print('\\nTest set score:', gbc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Stochastic Gradient Descent Scores----------------------\")\n",
    "print('Training set score:', SDG.score(X_train, y_train))\n",
    "print('\\nTest set score:', SDG.score(X_test, y_test))\n",
    "\n",
    "print(\"---------------------- Naive Bayes Multinominal Scores----------------------\")\n",
    "print('Training set score:', NB_Multi.score(X_train, y_train))\n",
    "print('\\nTest set score:', NB_Multi.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Naive Bayes Bernoulli Scores----------------------\")\n",
    "print('Training set score:', NB_Bern.score(X_train, y_train))\n",
    "print('\\nTest set score:', NB_Bern.score(X_test, y_test))"
   ]
  },
  {
   "source": [
    "# Conclustion\n",
    "\n",
    "Adding POS to Lemmatized features did not improve the result. And adding 2-grams to Lemmatized with POS feature gave same result as Lemmatized features alone. Above all models Naive Bayes Multinominal performed best. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}