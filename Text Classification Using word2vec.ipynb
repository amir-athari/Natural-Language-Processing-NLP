{
 "cells": [
  {
   "source": [
    "# Use of Word2Vec to classify documents"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Here we will use Jane Austen's *Persuasion* and Lewis Carroll's *Alice's Adventures in Wonderland* from NLTK's Gutenberg module. \n",
    "### The unit of observation (*documents*) will be the sentences of these novels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n[nltk_data]     C:\\Users\\00233270\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "import warnings\n",
    "from sklearn import (datasets, model_selection, feature_extraction, linear_model, naive_bayes, ensemble)\n",
    "import collections\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "import multiprocessing as mp \n",
    "#import textacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "#!python -m spacy download en\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#nlp = en_core_web_sm.load()"
   ]
  },
  {
   "source": [
    "### A helper function for removing some punctuation marks and numbers from the text:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# DO NOT DELETE ############################################# \n",
    "# Function to move specific column to the left side for easier view\n",
    "def move_to_left(df, column_name):\n",
    "    df= df[ [str(column_name)] + [ col for col in df.columns if col != str(column_name) ] ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the data\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cleaned texts are stored in two variables called `alice` and `persuasion`. Later, we will split the texts into sentences. We will use spaCy English module and use spaCy to parse both the `alice` and `persuasion` texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take some time.\n",
    "#nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text   author\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                                      (Oh, dear, !)  Carroll"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(So, she, was, considering, in, her, own, mind...</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(Oh, dear, !)</td>\n      <td>Carroll</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(Oh, dear, !)</td>\n      <td>Carroll</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# POSs\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one DataFrame\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
    "sentences.head()"
   ]
  },
  {
   "source": [
    "## Tokenize and Lemmaitize "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "lemma = []\n",
    "#pos = []\n",
    "\n",
    "for doc in nlp.pipe(sentences['text'].astype('unicode').values, batch_size=50,\n",
    "                        n_threads=3):\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc])\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "  #      pos.append([n.pos_ for n in doc])\n",
    "        \n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    " #       pos.append(None)\n",
    "\n",
    "sentences['tokens'] = tokens\n",
    "sentences['lemma'] = lemma\n",
    "#sentences['pos'] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text   author  \\\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll   \n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll   \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Alice, was, beginning, to, get, very, tired, ...   \n",
       "1  [So, she, was, considering, in, her, own, mind...   \n",
       "2  [There, was, nothing, so, VERY, remarkable, in...   \n",
       "\n",
       "                                               lemma  \n",
       "0  (Alice, be, begin, to, get, very, tired, of, s...  \n",
       "1  (so, -PRON-, be, consider, in, -PRON-, own, mi...  \n",
       "2  (there, be, nothing, so, very, remarkable, in,...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>author</th>\n      <th>tokens</th>\n      <th>lemma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n      <td>Carroll</td>\n      <td>[Alice, was, beginning, to, get, very, tired, ...</td>\n      <td>(Alice, be, begin, to, get, very, tired, of, s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(So, she, was, considering, in, her, own, mind...</td>\n      <td>Carroll</td>\n      <td>[So, she, was, considering, in, her, own, mind...</td>\n      <td>(so, -PRON-, be, consider, in, -PRON-, own, mi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n      <td>Carroll</td>\n      <td>[There, was, nothing, so, VERY, remarkable, in...</td>\n      <td>(there, be, nothing, so, very, remarkable, in,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "sentences.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences['lemma'] = [tuple(x) for x in sentences['lemma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas list to string\n",
    "#sentences['lemma'] = [', '.join(map(str, l)) for l in sentences['lemma']]\n",
    "#sentences['pos'] = [', '.join(map(str, l)) for l in sentences['pos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrd = sentences['lemma'].str.contains('peep').sum()\n",
    "if wrd>0:\n",
    "    print (\"There are {m}\".format(m=wrd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word2vec on the sentences\n",
    "model = gensim.models.Word2Vec(sentences[\"lemma\"], workers=4, min_count=1, window=6, sg=1, sample=1e-3, size=100, hs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('people', 0.7728192806243896), ('navy', 0.7627352476119995), ('gentleman', 0.7612074613571167), ('company', 0.7387996912002563), ('between', 0.7269482016563416)]\nuncle\n0.90191114\n0.51556206\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['lady', 'man'], negative=['woman'], topn=5))\n",
    "print(model.doesnt_match(\"dad dinner mom aunt uncle\".split()))\n",
    "print(model.similarity('woman', 'man'))\n",
    "print(model.similarity('horse', 'cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('Turtle', 0.2830783724784851), ('March', 0.1869412511587143), ('voice', 0.1750718355178833), ('Captain', 0.16483615338802338), ('tone', 0.16350901126861572)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['man'], negative=['woman'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('shoulder', 0.9129279851913452),\n",
       " ('knee', 0.9075636863708496),\n",
       " ('queen', 0.9064924716949463),\n",
       " ('toe', 0.905070424079895),\n",
       " ('farther', 0.9045087695121765)]"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5632, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text   author  \\\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll   \n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Alice, was, beginning, to, get, very, tired, ...   \n",
       "1  [So, she, was, considering, in, her, own, mind...   \n",
       "\n",
       "                                               lemma  \n",
       "0  (Alice, be, begin, to, get, very, tired, of, s...  \n",
       "1  (so, -PRON-, be, consider, in, -PRON-, own, mi...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>author</th>\n      <th>tokens</th>\n      <th>lemma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n      <td>Carroll</td>\n      <td>[Alice, was, beginning, to, get, very, tired, ...</td>\n      <td>(Alice, be, begin, to, get, very, tired, of, s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(So, she, was, considering, in, her, own, mind...</td>\n      <td>Carroll</td>\n      <td>[So, she, was, considering, in, her, own, mind...</td>\n      <td>(so, -PRON-, be, consider, in, -PRON-, own, mi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "sentences.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    author                                               text         0  \\\n",
       "0  Carroll  (Alice, was, beginning, to, get, very, tired, ...  0.158378   \n",
       "1  Carroll  (So, she, was, considering, in, her, own, mind...  0.172123   \n",
       "2  Carroll  (There, was, nothing, so, VERY, remarkable, in...  0.243648   \n",
       "3  Carroll                                      (Oh, dear, !)  0.042823   \n",
       "4  Carroll                                      (Oh, dear, !)  0.042823   \n",
       "\n",
       "          1         2         3         4         5         6         7  ...  \\\n",
       "0  0.060011  0.157750 -0.029606 -0.236574  0.036433  0.283540 -0.247457  ...   \n",
       "1  0.068418  0.066490  0.023262 -0.253577  0.043693  0.291482 -0.182503  ...   \n",
       "2  0.087739  0.124225 -0.016461 -0.293289 -0.024041  0.241044 -0.261307  ...   \n",
       "3  0.081108  0.248071  0.153290 -0.109163  0.080950  0.034149 -0.121746  ...   \n",
       "4  0.081108  0.248071  0.153290 -0.109163  0.080950  0.034149 -0.121746  ...   \n",
       "\n",
       "         90        91        92        93        94        95        96  \\\n",
       "0 -0.002104 -0.223912 -0.268862  0.030572  0.203090 -0.025319 -0.156174   \n",
       "1  0.002124 -0.208643 -0.264256  0.020851  0.184978  0.031838 -0.096466   \n",
       "2 -0.007751 -0.234767 -0.265950  0.011011  0.256305 -0.002414 -0.096413   \n",
       "3 -0.048959 -0.059532 -0.493349  0.311079  0.629674 -0.022445 -0.147822   \n",
       "4 -0.048959 -0.059532 -0.493349  0.311079  0.629674 -0.022445 -0.147822   \n",
       "\n",
       "         97        98        99  \n",
       "0 -0.392246 -0.055377 -0.251224  \n",
       "1 -0.332626 -0.098685 -0.210926  \n",
       "2 -0.379305 -0.086064 -0.248957  \n",
       "3 -0.627011 -0.269780 -0.358592  \n",
       "4 -0.627011 -0.269780 -0.358592  \n",
       "\n",
       "[5 rows x 102 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author</th>\n      <th>text</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Carroll</td>\n      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n      <td>0.158378</td>\n      <td>0.060011</td>\n      <td>0.157750</td>\n      <td>-0.029606</td>\n      <td>-0.236574</td>\n      <td>0.036433</td>\n      <td>0.283540</td>\n      <td>-0.247457</td>\n      <td>...</td>\n      <td>-0.002104</td>\n      <td>-0.223912</td>\n      <td>-0.268862</td>\n      <td>0.030572</td>\n      <td>0.203090</td>\n      <td>-0.025319</td>\n      <td>-0.156174</td>\n      <td>-0.392246</td>\n      <td>-0.055377</td>\n      <td>-0.251224</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Carroll</td>\n      <td>(So, she, was, considering, in, her, own, mind...</td>\n      <td>0.172123</td>\n      <td>0.068418</td>\n      <td>0.066490</td>\n      <td>0.023262</td>\n      <td>-0.253577</td>\n      <td>0.043693</td>\n      <td>0.291482</td>\n      <td>-0.182503</td>\n      <td>...</td>\n      <td>0.002124</td>\n      <td>-0.208643</td>\n      <td>-0.264256</td>\n      <td>0.020851</td>\n      <td>0.184978</td>\n      <td>0.031838</td>\n      <td>-0.096466</td>\n      <td>-0.332626</td>\n      <td>-0.098685</td>\n      <td>-0.210926</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Carroll</td>\n      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n      <td>0.243648</td>\n      <td>0.087739</td>\n      <td>0.124225</td>\n      <td>-0.016461</td>\n      <td>-0.293289</td>\n      <td>-0.024041</td>\n      <td>0.241044</td>\n      <td>-0.261307</td>\n      <td>...</td>\n      <td>-0.007751</td>\n      <td>-0.234767</td>\n      <td>-0.265950</td>\n      <td>0.011011</td>\n      <td>0.256305</td>\n      <td>-0.002414</td>\n      <td>-0.096413</td>\n      <td>-0.379305</td>\n      <td>-0.086064</td>\n      <td>-0.248957</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Carroll</td>\n      <td>(Oh, dear, !)</td>\n      <td>0.042823</td>\n      <td>0.081108</td>\n      <td>0.248071</td>\n      <td>0.153290</td>\n      <td>-0.109163</td>\n      <td>0.080950</td>\n      <td>0.034149</td>\n      <td>-0.121746</td>\n      <td>...</td>\n      <td>-0.048959</td>\n      <td>-0.059532</td>\n      <td>-0.493349</td>\n      <td>0.311079</td>\n      <td>0.629674</td>\n      <td>-0.022445</td>\n      <td>-0.147822</td>\n      <td>-0.627011</td>\n      <td>-0.269780</td>\n      <td>-0.358592</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Carroll</td>\n      <td>(Oh, dear, !)</td>\n      <td>0.042823</td>\n      <td>0.081108</td>\n      <td>0.248071</td>\n      <td>0.153290</td>\n      <td>-0.109163</td>\n      <td>0.080950</td>\n      <td>0.034149</td>\n      <td>-0.121746</td>\n      <td>...</td>\n      <td>-0.048959</td>\n      <td>-0.059532</td>\n      <td>-0.493349</td>\n      <td>0.311079</td>\n      <td>0.629674</td>\n      <td>-0.022445</td>\n      <td>-0.147822</td>\n      <td>-0.627011</td>\n      <td>-0.269780</td>\n      <td>-0.358592</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 102 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "word2vec_arr = np.zeros((sentences.shape[0],100))\n",
    "\n",
    "for i, sentence in enumerate(sentences[\"lemma\"]):\n",
    "    word2vec_arr[i,:] = np.mean([model[lemma] for lemma in sentence], axis=0)\n",
    "\n",
    "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
    "sentences = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr], axis=1)\n",
    "sentences.dropna(inplace=True)\n",
    "\n",
    "sentences.head()"
   ]
  },
  {
   "source": [
    "## Use multiple machine learning algorithms on 1-gram:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = sentences['author']\n",
    "X = np.array(sentences.drop(['text','author'], 1))\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "lr_params = {\"penalty\": [\"l2\"]}\n",
    "lr = LogisticRegression()\n",
    "\n",
    "rfc_params = {\"n_estimators\": [3, 5, 10, 15], \"max_depth\": [2, 3, 4, 5], \"min_samples_split\": [3, 5, 7, 9]}\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "gbc_params = {\"n_estimators\": [3, 5, 10, 15],\"max_depth\": [2, 3, 4, 5], \"min_samples_split\": [3, 5, 7, 9]}\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "SDG_params= {'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e3], 'penalty': ['l2'], 'n_jobs': [-1]}\n",
    "SDG = linear_model.SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, estimator=SGDClassifier(),\n",
       "             param_grid={'alpha': [0.0001, 0.001, 0.01, 0.1, 1000.0],\n",
       "                         'n_jobs': [-1], 'penalty': ['l2']})"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "# GridsearchCV\n",
    "clf_lr = GridSearchCV(lr, lr_params, cv=2)\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "clf_rfc = GridSearchCV(rfc, rfc_params, cv=2)\n",
    "clf_rfc.fit(X_train, y_train)\n",
    "\n",
    "clf_gbc = GridSearchCV(gbc, gbc_params, cv=2)\n",
    "clf_gbc.fit(X_train, y_train)\n",
    "\n",
    "clf_SDG = GridSearchCV(SDG, SDG_params, cv=2)\n",
    "clf_SDG.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "# Fit the models\n",
    "lr.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "SDG.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------Logistic Regression Scores----------------------\nTraining set score: 0.9274933412252145\n\nTest set score: 0.9289835774522859\n----------------------Random Forest Scores----------------------\nTraining set score: 0.999408108907961\n\nTest set score: 0.9369729249889037\n----------------------Gradient Boosting Scores----------------------\nTraining set score: 0.977212192956496\n\nTest set score: 0.9445184198845983\n----------------------Stochastic Gradient Descent Scores----------------------\nTraining set score: 0.940514945250074\n\nTest set score: 0.9422991566799822\n"
     ]
    }
   ],
   "source": [
    "# Check some basic performance\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Random Forest Scores----------------------\")\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "print('Training set score:', gbc.score(X_train, y_train))\n",
    "print('\\nTest set score:', gbc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Stochastic Gradient Descent Scores----------------------\")\n",
    "print('Training set score:', SDG.score(X_train, y_train))\n",
    "print('\\nTest set score:', SDG.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}