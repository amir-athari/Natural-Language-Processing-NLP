{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cornell Movie Dialogs ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will develop a simple chatbot by training it on Cornell Movie Dialogs corpus containing a large metadata-rich collection of fictional conversations extracted from raw movie scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data: http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\00233270\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (51.0.0.post20201207)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.55.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.2)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\00233270\\anaconda3\\envs\\py37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "symbolic link created for C:\\Users\\00233270\\Anaconda3\\envs\\py37\\lib\\site-packages\\spacy\\data\\en <<===>> C:\\Users\\00233270\\Anaconda3\\envs\\py37\\lib\\site-packages\\en_core_web_sm\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "[+] Linking successful\n",
      "C:\\Users\\00233270\\Anaconda3\\envs\\py37\\lib\\site-packages\\en_core_web_sm -->\n",
      "C:\\Users\\00233270\\Anaconda3\\envs\\py37\\lib\\site-packages\\spacy\\data\\en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import gutenberg\n",
    "import re\n",
    "import spacy\n",
    "import warnings\n",
    "from sqlalchemy import create_engine\n",
    "from chatterbot import ChatBot\n",
    "from chatterbot.trainers import ListTrainer, ChatterBotCorpusTrainer\n",
    "from chatterbot.conversation import Statement\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_user = 'dsbc_student'\n",
    "postgres_pw = '7*.8G9QH21'\n",
    "postgres_host = '142.93.121.174'\n",
    "postgres_port = '5432'\n",
    "postgres_db = 'cornell_movie_dialogs'\n",
    "\n",
    "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(\n",
    "    postgres_user, postgres_pw, postgres_host, postgres_port, postgres_db))\n",
    "\n",
    "df0 = pd.read_sql_query('select * from dialogs', con=engine)\n",
    "\n",
    "# no need for an open connection, \n",
    "# as we're only doing a single query\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 304446 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "nRow, nCol = df0.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>dialogs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Can we make this quick?  Roxanne Korrine and A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Not the hacking and gagging and spitting part....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Okay... then how 'bout we try out some French ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            dialogs\n",
       "0      0  Can we make this quick?  Roxanne Korrine and A...\n",
       "1      1  Well, I thought we'd start with pronunciation,...\n",
       "2      2  Not the hacking and gagging and spitting part....\n",
       "3      3  Okay... then how 'bout we try out some French ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df0.sample(200000)\n",
    "#df1 = df0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--','',text)\n",
    "    text = re.sub(\"[\\[]*[\\]]\", \"\", text)\n",
    "    text = re.sub(\"[\\[]*[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1['cleaned'] = df1['dialogs'].astype(str).apply(text_cleaner)\n",
    "# df1['tokens'] = df1['cleaned'].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11030388"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the text in column to a body of text\n",
    "dialogs = df1['dialogs'].tolist()\n",
    "dialogs_doc = ''.join(dialogs)\n",
    "len(dialogs_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 12000000 # or even higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fb3a56837bd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdialogs_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdialogs_doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "dialogs_doc = nlp(dialogs_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break the body to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs_sents = [sent.text for sent in dialogs_doc.sents if len(sent.text) > 1]\n",
    "dialogs_sents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can create our own chatbot and train it using this corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chatbot\n",
    "chatbot = ChatBot('MovieExpert')\n",
    "# This is to remove the accumulated knowledge base\n",
    "chatbot.storage.drop()\n",
    "\n",
    "# Create a new trainer for the chatbot\n",
    "trainer = ListTrainer(chatbot)\n",
    "\n",
    "# Train the chatbot based on Persuasion\n",
    "trainer.train(dialogs_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we create a function to engage the user if the text contains any greeting words. And if it contains one of them, the chatbot will respond with another greeting word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = [\"hello\", \"hi\", \"greetings\", \"what's up\",\"hey\"]\n",
    "GREETING_RESPONSES = [\"hello\", \"hi\", \"hey\", \"hi there\"]\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we run the chatpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MovieExpert: I will try to respond to you reasonably. If you want to exit, type bye.\")\n",
    "\n",
    "# Below is the chatting\n",
    "while True:\n",
    "    \n",
    "    user_input = input(\"User: \")\n",
    "    user_input=user_input.lower()\n",
    "    \n",
    "    if(user_input!='bye'):\n",
    "        if(user_input == 'thanks' or user_input == 'thank you'):\n",
    "            break\n",
    "            print(\"MovieExpert: You're welcome.\")\n",
    "        else:\n",
    "            if(greeting(user_input) != None):\n",
    "                print(\"MovieExpert: \" + greeting(user_input))\n",
    "            else:\n",
    "                print(\"MovieExpert: \", end = \"\")\n",
    "                print(chatbot.get_response(user_input))\n",
    "    else:\n",
    "        print(\"MovieExpert: Bye! It was a great chat.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = []\n",
    "# lemma = []\n",
    "# #pos = []\n",
    "\n",
    "# for doc in nlp.pipe(df1['dialogs'].astype('unicode').values, batch_size=50,\n",
    "#                         n_threads=3):\n",
    "#     if doc.is_parsed:\n",
    "#         tokens.append([n.text for n in doc])\n",
    "#         lemma.append([n.lemma_ for n in doc])\n",
    "# #        pos.append([n.pos_ for n in doc])\n",
    "        \n",
    "#     else:\n",
    "#         # We want to make sure that the lists of parsed results have the\n",
    "#         # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "#         tokens.append(None)\n",
    "#         lemma.append(None)\n",
    "# #        pos.append(None)\n",
    "\n",
    "# df1['tokens'] = tokens\n",
    "# df1['lemma'] = lemma\n",
    "# #df1['pos'] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can create our own chatbot and train it using Persuasion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chatbot\n",
    "c.\n",
    "hatbot = ChatBot('Persuasion')\n",
    "# This is to remove the accumulated knowledge base\n",
    "chatbot.storage.drop()\n",
    "\n",
    "# Create a new trainer for the chatbot\n",
    "trainer = ListTrainer(chatbot)\n",
    "\n",
    "# Train the chatbot based on Persuasion\n",
    "trainer.train(persuasion_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, run the chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Persuasion: I will try to respond to you reasonably. If you want to exit, type bye.\")\n",
    "\n",
    "# Below is the chatting\n",
    "while True:\n",
    "    \n",
    "    user_input = input(\"User: \")\n",
    "    user_input=user_input.lower()\n",
    "    \n",
    "    if(user_input!='bye'):\n",
    "        if(user_input == 'thanks' or user_input == 'thank you'):\n",
    "            break\n",
    "            print(\"Persuasion: You're welcome.\")\n",
    "        else:\n",
    "            if(greeting(user_input) != None):\n",
    "                print(\"Persuasion: \" + greeting(user_input))\n",
    "            else:\n",
    "                print(\"Persuasion: \", end = \"\")\n",
    "                print(chatbot.get_response(user_input))\n",
    "    else:\n",
    "        print(\"Persuasion: Bye! It was a great chat.\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
