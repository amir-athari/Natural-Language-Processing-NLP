{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    },
    "colab": {
      "name": "Cornell Movie Dialogs Chatbot.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKDsN6Wz7l25"
      },
      "source": [
        "# Twitter US Airline Sentiment Text Generation By Markov Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8rTDCHw7l2_"
      },
      "source": [
        "### Here we will generate random and simple sentences based on two criteria:\n",
        "1. They should be grammatically correct.\n",
        "2. They should make sense—or at least some sense!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-t-teZP7l3A"
      },
      "source": [
        "### Data: https://www.kaggle.com/crowdflower/twitter-airline-sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIZlTV3l7l3A",
        "outputId": "694612f9-6a50-46dd-e757-77cfe6ba202c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import string\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import gutenberg\n",
        "import re\n",
        "import spacy\n",
        "import warnings\n",
        "import markovify\n",
        "from sqlalchemy import create_engine\n",
        "#from chatterbot import ChatBot\n",
        "#from chatterbot.trainers import ListTrainer, ChatterBotCorpusTrainer\n",
        "#from chatterbot.conversation import Statement\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('gutenberg')\n",
        "!python -m spacy download en"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (2.3.1)\n",
            "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
            "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.0.post20200714)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
            "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.47.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.9.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.9)\n",
            "✔ Download and installation successful\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "✘ Couldn't link model to 'en'\n",
            "Creating a symlink in spacy/data failed. Make sure you have the required\n",
            "permissions and try re-running the command as admin, or use a virtualenv. You\n",
            "can still import the model as a module and call its load() method, or create the\n",
            "symlink manually.\n",
            "C:\\Users\\User\\anaconda3\\lib\\site-packages\\en_core_web_sm -->\n",
            "C:\\Users\\User\\anaconda3\\lib\\site-packages\\spacy\\data\\en\n",
            "⚠ Download successful but linking failed\n",
            "Creating a shortcut link for 'en' didn't work (maybe you don't have admin\n",
            "permissions?), but you can still load the model via its full package name: nlp =\n",
            "spacy.load('en_core_web_sm')\n",
            "You do not have sufficient privilege to perform this operation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbthOX2x7l3B"
      },
      "source": [
        "## Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ4j3COk7l3C"
      },
      "source": [
        "postgres_user = 'dsbc_student'\n",
        "postgres_pw = '7*.8G9QH21'\n",
        "postgres_host = '142.93.121.174'\n",
        "postgres_port = '5432'\n",
        "postgres_db = 'twitter_sentiment'\n",
        "\n",
        "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(\n",
        "    postgres_user, postgres_pw, postgres_host, postgres_port, postgres_db))\n",
        "\n",
        "df0 = pd.read_sql_query('select * from twitter', con=engine)\n",
        "\n",
        "# no need for an open connection, \n",
        "# as we're only doing a single query\n",
        "engine.dispose()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5NCTznm7l3C",
        "outputId": "d2812653-2f99-490a-fa94-77daf157afd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nRow, nCol = df0.shape\n",
        "print(f'There are {nRow} rows and {nCol} columns')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 14640 rows and 16 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrUvv6zh7l3C",
        "outputId": "2679a5ec-90d8-4b3f-fc18-b8f765441114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "df0.head(2)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index            tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
              "0      0  570306133677760513           neutral                        1.0000   \n",
              "1      1  570301130888122368          positive                        0.3486   \n",
              "\n",
              "  negativereason  negativereason_confidence         airline  \\\n",
              "0           None                        NaN  Virgin America   \n",
              "1           None                        0.0  Virgin America   \n",
              "\n",
              "  airline_sentiment_gold      name negativereason_gold  retweet_count  \\\n",
              "0                   None   cairdin                None              0   \n",
              "1                   None  jnardino                None              0   \n",
              "\n",
              "                                                text tweet_coord  \\\n",
              "0                @VirginAmerica What @dhepburn said.        None   \n",
              "1  @VirginAmerica plus you've added commercials t...        None   \n",
              "\n",
              "               tweet_created tweet_location               user_timezone  \n",
              "0  2015-02-24 11:35:52 -0800           None  Eastern Time (US & Canada)  \n",
              "1  2015-02-24 11:15:59 -0800           None  Pacific Time (US & Canada)  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>tweet_id</th>\n      <th>airline_sentiment</th>\n      <th>airline_sentiment_confidence</th>\n      <th>negativereason</th>\n      <th>negativereason_confidence</th>\n      <th>airline</th>\n      <th>airline_sentiment_gold</th>\n      <th>name</th>\n      <th>negativereason_gold</th>\n      <th>retweet_count</th>\n      <th>text</th>\n      <th>tweet_coord</th>\n      <th>tweet_created</th>\n      <th>tweet_location</th>\n      <th>user_timezone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>570306133677760513</td>\n      <td>neutral</td>\n      <td>1.0000</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>Virgin America</td>\n      <td>None</td>\n      <td>cairdin</td>\n      <td>None</td>\n      <td>0</td>\n      <td>@VirginAmerica What @dhepburn said.</td>\n      <td>None</td>\n      <td>2015-02-24 11:35:52 -0800</td>\n      <td>None</td>\n      <td>Eastern Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>570301130888122368</td>\n      <td>positive</td>\n      <td>0.3486</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>Virgin America</td>\n      <td>None</td>\n      <td>jnardino</td>\n      <td>None</td>\n      <td>0</td>\n      <td>@VirginAmerica plus you've added commercials t...</td>\n      <td>None</td>\n      <td>2015-02-24 11:15:59 -0800</td>\n      <td>None</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24_QsTSI7l3D"
      },
      "source": [
        "# Let's focus on negative and positive sentiments seperately\n",
        "df1 = df0.sample(4000)\n",
        "df1_neg = df1[df1['airline_sentiment'] == 'negative']\n",
        "df1_pos = df1[df1['airline_sentiment'] == 'positive']\n",
        "#df1_pos = df0.copy()"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqqYprrG7l3D",
        "outputId": "c5807e0a-323b-460d-faf3-5f1c4040351d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df1_neg.shape, df1_pos.shape"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2522, 16), (620, 16))"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0Oo5G7O7l3E"
      },
      "source": [
        "# Utility function for standard text cleaning\n",
        "def text_cleaner(text):\n",
        "    text = re.sub(r'--','',text)\n",
        "    text = re.sub(\"[\\[]*[\\]]\", \"\", text)\n",
        "    text = re.sub(\"\\@\", \"\", text)\n",
        "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
        "    text = re.sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)\", \"\", text) # remove links\n",
        "    text = ' '.join(text.split())\n",
        "    return text"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N237TuCJ7l3E"
      },
      "source": [
        "df1_pos['cleaned'] = df1_pos['text'].astype(str).apply(text_cleaner)\n",
        "df1_neg['cleaned'] = df1_neg['text'].astype(str).apply(text_cleaner)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUVITZcN7l3F",
        "outputId": "78d1fa7c-a3e1-4b0e-db70-fc041050ba65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Convert the text in column to a body of text\n",
        "dialogs_neg = df1_neg['cleaned'].tolist()\n",
        "dialogs_neg_doc = ''.join(dialogs_neg)\n",
        "\n",
        "# Convert the text in column to a body of text\n",
        "dialogs_pos = df1_pos['cleaned'].tolist()\n",
        "dialogs_pos_doc = ''.join(dialogs_pos)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(272869, 52720)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "len(dialogs_neg_doc), len(dialogs_pos_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "type(dialogs_neg_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to remove emojis\n",
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove emojis\n",
        "dialogs_neg_doc = deEmojify(dialogs_neg_doc)\n",
        "dialogs_pos_doc = deEmojify(dialogs_pos_doc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JQ6GVbY7l3F"
      },
      "source": [
        "# Adust NLP for a large body of text\n",
        "nlp.max_length = 6000000 # or even higher\n",
        "dialogs_neg_doc = nlp(dialogs_neg_doc)\n",
        "dialogs_pos_doc = nlp(dialogs_pos_doc)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFIcOQ847l3F"
      },
      "source": [
        "## Break the body to sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoPtJ4Xo7l3G",
        "outputId": "3acbba8f-3a7a-43bc-ea18-ca7c7de3bc51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dialogs_neg_sents = [sent.text for sent in dialogs_neg_doc.sents if len(sent.text) > 1]\n",
        "dialogs_pos_sents = [sent.text for sent in dialogs_pos_doc.sents if len(sent.text) > 1]"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "source": [
        "### To generate the transition probabilities, we wil use Markovify's `Text()` class. This class has a parameter called `state_size`. This parameter determines how many words the model uses as the current state. For example, if we want to generate the next word by looking at just the previous word, set `state_size=1`. If we want to generate the next word by looking at the previous two words, then set `state_size=2`. The following is set to `state_size=3`."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "dialogs_neg_generator = markovify.Text(dialogs_neg_sents, state_size = 3)\n",
        "dialogs_pos_generator = markovify.Text(dialogs_pos_sents, state_size = 3)"
      ]
    },
    {
      "source": [
        "### At this stage, we've trained a Markov chain model from *twitter*. Now, we're all set to generate random sentences from this model:"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\nNone\ngive you $ voucher, like I want to put a coat in there isn't that my choice?\n, I'm so sad for the baggage to begin loading.\nI think it's safe to say that JetBlue has officially lost a customer.\nbut i should still be able to complete the trip.\n"
          ]
        }
      ],
      "source": [
        "# Three randomly generated negative sentences\n",
        "for i in range(3):\n",
        "    print(dialogs_neg_generator.make_sentence())\n",
        "\n",
        "# Three randomly generated sentences of no more than 100 characters\n",
        "for i in range(3):\n",
        "    print(dialogs_neg_generator.make_short_sentence(100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\nNone\nNone\nI knew there was a frequent tweeter discount\nThank you!SouthwestAir thank you for contacting me.\nthe entire flight crew on flight from Orlando to Indy was AMAZING!\n"
          ]
        }
      ],
      "source": [
        "# Three randomly generated positive sentences\n",
        "for i in range(3):\n",
        "    print(dialogs_pos_generator.make_sentence())\n",
        "\n",
        "# Three randomly generated sentences of no more than 100 characters\n",
        "for i in range(3):\n",
        "    print(dialogs_pos_generator.make_short_sentence(100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "source": [
        "## The sentences do sound good but not very natural and to improve the performance of the model, we can use some syntactic information like part-of-speech tags. The Markovify package also supports this and can work together with spaCy as follows:"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "class POSifiedText(markovify.Text):\n",
        "    \n",
        "    def word_split(self, sentence):\n",
        "        return [\"::\".join((word.orth_, word.pos_)) for word in nlp(sentence)]\n",
        "\n",
        "    def word_join(self, words):\n",
        "        sentence = \" \".join(word.split(\"::\")[0] for word in words)\n",
        "        return sentence"
      ]
    },
    {
      "source": [
        "### Now, train a Markov chain model again. This time, use the `POSifiedText()` class:"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "dialogs_neg_generator = POSifiedText(dialogs_neg_sents, state_size = 3)\n",
        "dialogs_pos_generator = POSifiedText(dialogs_pos_sents, state_size = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AmericanAir has the worst flights and customer service , irresponsible staff and lack of snacks on the plane .\nThe pilot told us they would release bags as well as other passengers tickets away .\nNone\nBut not everyone is on the ground .....\nNo window at all .\nWas supposed to be in a flight ..\n"
          ]
        }
      ],
      "source": [
        "# Three randomly generated negative sentences\n",
        "for i in range(3):\n",
        "    print(dialogs_neg_generator.make_sentence())\n",
        "\n",
        "# Three randomly generated sentences of no more than 100 characters\n",
        "for i in range(3):\n",
        "    print(dialogs_neg_generator.make_short_sentence(100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\nYou guys are awesome !\nNone\nfavoriteairline # luvforSW # southwestAirunited and to add to reasons why I fly with you\nThank you!SouthwestAir thank you for the credit .\nSouthwestAir - I just had a great LA flight with Clarence and Frank !\n"
          ]
        }
      ],
      "source": [
        "# Three randomly generated positive sentences\n",
        "for i in range(3):\n",
        "    print(dialogs_pos_generator.make_sentence())\n",
        "\n",
        "# Three randomly generated sentences of no more than 100 characters\n",
        "for i in range(3):\n",
        "    print(dialogs_pos_generator.make_short_sentence(100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "source": [
        "## These generators work very well. Both negative and positve texts show meaninful semantic and syntax."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}