{
 "cells": [
  {
   "source": [
    "# Use of TF-IDF on large dataset for string matching"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## There are many fuzzy matching algorithms that work fine on small dataset. However, they fall short when used on even modest data sets of greater than a few thousand records. This is because they compare each record to all the other records in the data set. Here we will use TF_IDF to compare list of 3000 names in a lookup database of around 1 million names."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import plot, xlabel, ylabel\n",
    "%matplotlib inline\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.patches import PathPatch\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import matplotlib.cm as cm\n",
    "from IPython.core.display import display\n",
    "import matplotlib.patches as patches\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "from numpy.random import seed\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime as dt\n",
    "import time\n",
    "import spacy\n",
    "import re\n",
    "import pyodbc\n",
    "import sqlalchemy as sal\n",
    "from sklearn.preprocessing import normalize \n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from scipy.stats import bartlett\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import normalize \n",
    "from scipy.stats import jarque_bera\n",
    "from scipy.stats import levene\n",
    "from scipy.stats import normaltest\n",
    "import scipy.stats as stats\n",
    "from scipy.stats.mstats import winsorize\n",
    "from scipy.stats import zscore\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score #\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import xlsxwriter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, \\\n",
    "    adjusted_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_samples,  silhouette_score\n",
    "from sklearn.metrics.cluster import contingency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import en_core_web_md\n",
    "# nlp = spacy.load('en_core_web_md')\n",
    "from random import sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100) \n",
    "pd.set_option('display.max_colwidth', -1) "
   ]
  },
  {
   "source": [
    "## Import two data sources"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File D:\\dev_data\\re\\hcad\\pro_names.csv does not exist: 'D:\\\\dev_data\\\\re\\\\hcad\\\\pro_names.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-cc63a5a09059>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfile_name1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'pro_names.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfile1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\\\\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mfile_name1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdfp0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdfp0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File D:\\dev_data\\re\\hcad\\pro_names.csv does not exist: 'D:\\\\dev_data\\\\re\\\\hcad\\\\pro_names.csv'"
     ]
    }
   ],
   "source": [
    "## import the query data\n",
    "import_path = r'D:\\dev_data\\re\\hcad'\n",
    "file_name1 = 'pro_names.csv'\n",
    "file1 = import_path+\"\\\\\"+ file_name1\n",
    "dfp0 = pd.read_csv(file1)\n",
    "dfp0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0  DOCK_NUM          DE_NAME DE_FIRST_NAME DE_MIDDLE_NAME  \\\n",
       "0  0           471514    CAROLYN A BROWN  CAROLYN       A               \n",
       "1  1           485685    LUCIO SOLIS      LUCIO         NaN             \n",
       "\n",
       "  DE_LAST_NAME  de_prop_given  \n",
       "0  BROWN        0              \n",
       "1  SOLIS        1              "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>DOCK_NUM</th>\n      <th>DE_NAME</th>\n      <th>DE_FIRST_NAME</th>\n      <th>DE_MIDDLE_NAME</th>\n      <th>DE_LAST_NAME</th>\n      <th>de_prop_given</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>471514</td>\n      <td>CAROLYN A BROWN</td>\n      <td>CAROLYN</td>\n      <td>A</td>\n      <td>BROWN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>485685</td>\n      <td>LUCIO SOLIS</td>\n      <td>LUCIO</td>\n      <td>NaN</td>\n      <td>SOLIS</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "dfp0.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1087036, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Import lookup data\n",
    "owner_cols = ['ACCOUNT', 'MAILTO']\n",
    "file_name2 = 'ss_owners.csv'\n",
    "file2 = import_path+\"\\\\\"+ file_name2\n",
    "dfo0 = pd.read_csv(file2,  dtype= str, encoding = \"ISO-8859-1\", names=owner_cols, skiprows=1)\n",
    "dfo0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         ACCOUNT                  MAILTO\n",
       "0  0032180000021  SANTOS DOLORES ST JOHN\n",
       "1  0032180000022  GRIMALDO ROSIE        \n",
       "2  0032180000023  GARCIA ANTONIO        "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ACCOUNT</th>\n      <th>MAILTO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0032180000021</td>\n      <td>SANTOS DOLORES ST JOHN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0032180000022</td>\n      <td>GRIMALDO ROSIE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0032180000023</td>\n      <td>GARCIA ANTONIO</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dfo0.head(3)"
   ]
  },
  {
   "source": [
    "## Helper functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'\"','',text)\n",
    "    text = re.sub(r'&','',text)\n",
    "    ext = re.sub(\"[\\[].*?[\\]];\", \"\", text)\n",
    "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "source": [
    "## Preprocess lookup dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo1 = dfo0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['santos dolores st john', 'grimaldo rosie', 'garcia antonio']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Convert column of names to string and clean\n",
    "l = dfo1['MAILTO'].tolist() \n",
    "l=['missing' if x is np.nan else x for x in l]\n",
    "s = '||||'.join(l).lower()\n",
    "sc = text_cleaner(s)\n",
    "#sc[:10]\n",
    "names = [str(x) for x in sc.split('||||') if x]\n",
    "names[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           MAILTO_cleaned\n",
       "0  santos dolores st john\n",
       "1  grimaldo rosie        \n",
       "2  garcia antonio        "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MAILTO_cleaned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>santos dolores st john</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>grimaldo rosie</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>garcia antonio</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "dfo2 = pd.DataFrame(names, index=dfo1.index, columns=['MAILTO_cleaned'])\n",
    "dfo2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1087036, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Combine original df with cleaned names\n",
    "dfo3 = pd.concat([dfo1, dfo2], axis=1)\n",
    "dfo3['source'] = 'owners' # Add this so can identify the array later\n",
    "dfo3['de_prop_given'] = '' # Placeholder for future use\n",
    "dfo3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         ACCOUNT                  MAILTO          MAILTO_cleaned  source  \\\n",
       "0  0032180000021  SANTOS DOLORES ST JOHN  santos dolores st john  owners   \n",
       "1  0032180000022  GRIMALDO ROSIE          grimaldo rosie          owners   \n",
       "2  0032180000023  GARCIA ANTONIO          garcia antonio          owners   \n",
       "\n",
       "  de_prop_given  \n",
       "0                \n",
       "1                \n",
       "2                "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ACCOUNT</th>\n      <th>MAILTO</th>\n      <th>MAILTO_cleaned</th>\n      <th>source</th>\n      <th>de_prop_given</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0032180000021</td>\n      <td>SANTOS DOLORES ST JOHN</td>\n      <td>santos dolores st john</td>\n      <td>owners</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0032180000022</td>\n      <td>GRIMALDO ROSIE</td>\n      <td>grimaldo rosie</td>\n      <td>owners</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0032180000023</td>\n      <td>GARCIA ANTONIO</td>\n      <td>garcia antonio</td>\n      <td>owners</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "dfo3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1087036, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "dfo3 = dfo3.drop_duplicates(subset='ACCOUNT', keep=\"last\")\n",
    "dfo3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag commercial names by regex pattern\n",
    "def find_pat(text):\n",
    "    if re.search(r\" llc|current owner|  inc| lc| ltd| lp| churchcorp| company|city of houston|\\\n",
    "        county of harris|state of texas| company| harris county|harris county|county of harris|\\\n",
    "            texas department| city of katy|parcel\", text):\n",
    "        return 1\n",
    "    return   0\n",
    "# Applu the function\n",
    "dfo3['non_person'] = dfo3['MAILTO_cleaned'].apply(find_pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "68266"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Number of commercial entries\n",
    "dfo3['non_person'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1018770, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Drop commercial entries\n",
    "dfo3 = dfo3.drop(dfo3[dfo3.non_person ==1].index)\n",
    "dfo3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              ACCOUNT                      MAILTO            MAILTO_cleaned  \\\n",
       "873629  1251620060091  CHEN VAN                    chen van                   \n",
       "379310  1012910000022  SULLIVAN WILLIAM & BARBARA  sullivan william barbara   \n",
       "66471   0440660000276  ZARSKY WESLEY               zarsky wesley              \n",
       "\n",
       "        source de_prop_given  non_person    l_name  \n",
       "873629  owners                0           chen      \n",
       "379310  owners                0           sullivan  \n",
       "66471   owners                0           zarsky    "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ACCOUNT</th>\n      <th>MAILTO</th>\n      <th>MAILTO_cleaned</th>\n      <th>source</th>\n      <th>de_prop_given</th>\n      <th>non_person</th>\n      <th>l_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>873629</th>\n      <td>1251620060091</td>\n      <td>CHEN VAN</td>\n      <td>chen van</td>\n      <td>owners</td>\n      <td></td>\n      <td>0</td>\n      <td>chen</td>\n    </tr>\n    <tr>\n      <th>379310</th>\n      <td>1012910000022</td>\n      <td>SULLIVAN WILLIAM &amp; BARBARA</td>\n      <td>sullivan william barbara</td>\n      <td>owners</td>\n      <td></td>\n      <td>0</td>\n      <td>sullivan</td>\n    </tr>\n    <tr>\n      <th>66471</th>\n      <td>0440660000276</td>\n      <td>ZARSKY WESLEY</td>\n      <td>zarsky wesley</td>\n      <td>owners</td>\n      <td></td>\n      <td>0</td>\n      <td>zarsky</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Extract the last name\n",
    "dfo3['l_name'] = dfo3['MAILTO_cleaned'].str.extract('^([\\w\\-]+)', expand=True)\n",
    "dfo3 = dfo3[~dfo3['l_name'].isnull()] # Filter away those names that start with digits\n",
    "dfo3.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1018347, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "dfo3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1018347, 7), (1018347, 7))"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "dfo31 = dfo3.copy()\n",
    "#dfo31 = dfo3[:10000]\n",
    "dfo3.shape, dfo31.shape"
   ]
  },
  {
   "source": [
    "# ignore this section"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert column of names to string and clean\n",
    "# lst = dfp0['DE_LAST_NAME'].tolist()\n",
    "# lst_str = '||||'.join(lst).lower()\n",
    "# p_lnames = [str(x) for x in lst_str.split('||||') if x] \n",
    "# p_lnames.sort()\n",
    "\n",
    "# # Remove duplicates\n",
    "# res = [] \n",
    "# [res.append(x) for x in p_lnames if x not in res] \n",
    "# p_lnames = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create regex mask for last names\n",
    "# lname_empty = []\n",
    "# for count, value in enumerate(p_lnames):\n",
    "#     lname_empty.append(\"((?:\\s|^)\"+value+\"(?:\\s|$))\") # Regex get a whole word and not partial in any part\n",
    "\n",
    "# lname_reg_str = '|'.join(lname_empty)\n",
    "# lname_pat = re.compile(lname_reg_str) #'^\\\\b'+reg_str+'\\\\b')\n",
    "# lname_reg_str[:100]"
   ]
  },
  {
   "source": [
    "### First name regex processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert column of names to string and clean\n",
    "# lst = dfp0['DE_FIRST_NAME'].tolist()\n",
    "# lst_str = '||||'.join(lst).lower()\n",
    "# p_fnames = [str(x) for x in lst_str.split('||||') if x] \n",
    "# p_fnames.sort()\n",
    "\n",
    "# # Remove duplicates\n",
    "# res = [] \n",
    "# [res.append(x) for x in p_fnames if x not in res] \n",
    "# p_fnames = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create regex mask for first names\n",
    "# fname_empty = []\n",
    "# for count, value in enumerate(p_fnames):\n",
    "#     fname_empty.append(\"((?:\\s|^)\"+value+\"(?:\\s|$))\") # Regex get a whole word and not partial in any part\n",
    "\n",
    "# fname_reg_str = '|'.join(fname_empty)\n",
    "# fname_pat = re.compile(fname_reg_str) #'^\\\\b'+reg_str+'\\\\b')"
   ]
  },
  {
   "source": [
    "## This section is preparation for using with Dask in case need parallelization later"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove null to can save as parquet\n",
    "# dfo3 = dfo3[dfo3['MAILTO'].notna()]\n",
    "# dfo3 = dfo3[dfo3['l_name'].notna()]\n",
    "# dfo3['source'] = 'owners'\n",
    "# dfo3 = dfo3.astype(str)\n",
    "# dfo3 = dfo3.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check to see how frequent last names are\n",
    "# from collections import Counter\n",
    "# wrd_lst = dfo31['l_name'].tolist()\n",
    "# words_to_count = (word for word in wrd_lst)\n",
    "# c = Counter(words_to_count)\n",
    "# print (c.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# import pyarrow.parquet as pq\n",
    "# import dask.dataframe as dd\n",
    "# from dask.diagnostics import ProgressBar\n",
    "# %matplotlib inline\n",
    "\n",
    "# # Save owner data as parquet\n",
    "# export_path = r'D:\\\\dev_data\\\\re\\\\hcad\\\\lookup_data\\\\'\n",
    "# ddf = dd.from_pandas(dfo31, chunksize=1000)\n",
    "# save_dir = export_path\n",
    "# ddf.to_parquet(save_dir)"
   ]
  },
  {
   "source": [
    "# Ignore before here"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Preprocess querry dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp1 = dfp0[['DOCK_NUM', 'DE_NAME', 'de_prop_given']]\n",
    "dfp1 = dfp1.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "dfp1['source'] = 'prob' # Add this so can identify the array later\n",
    "dfp1 = dfp1.rename(columns={\"DE_NAME\": \"MAILTO_cleaned\", \"DOCK_NUM\": \"ACCOUNT\"})\n",
    "# Remove pro duplicates\n",
    "dfp1 = dfp1.drop_duplicates(subset='MAILTO_cleaned', keep=\"last\")\n",
    "#dfp1 = dfp1[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      ACCOUNT        MAILTO_cleaned  de_prop_given source      l_name\n",
       "1118  490206   maude a jones         0              prob   jones     \n",
       "1660  487813   marguerite baldwin    0              prob   baldwin   \n",
       "1611  487691   mohammad taha         0              prob   taha      \n",
       "1259  489282   claudette whitefield  1              prob   whitefield\n",
       "2232  488611   donald l connelly     0              prob   connelly  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ACCOUNT</th>\n      <th>MAILTO_cleaned</th>\n      <th>de_prop_given</th>\n      <th>source</th>\n      <th>l_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1118</th>\n      <td>490206</td>\n      <td>maude a jones</td>\n      <td>0</td>\n      <td>prob</td>\n      <td>jones</td>\n    </tr>\n    <tr>\n      <th>1660</th>\n      <td>487813</td>\n      <td>marguerite baldwin</td>\n      <td>0</td>\n      <td>prob</td>\n      <td>baldwin</td>\n    </tr>\n    <tr>\n      <th>1611</th>\n      <td>487691</td>\n      <td>mohammad taha</td>\n      <td>0</td>\n      <td>prob</td>\n      <td>taha</td>\n    </tr>\n    <tr>\n      <th>1259</th>\n      <td>489282</td>\n      <td>claudette whitefield</td>\n      <td>1</td>\n      <td>prob</td>\n      <td>whitefield</td>\n    </tr>\n    <tr>\n      <th>2232</th>\n      <td>488611</td>\n      <td>donald l connelly</td>\n      <td>0</td>\n      <td>prob</td>\n      <td>connelly</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# Extract the last name\n",
    "dfp1['l_name'] = dfp1['MAILTO_cleaned'].str.extract('([\\w\\-]+)$', expand=True)\n",
    "dfp1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop extra columns\n",
    "dfo31 = dfo31.drop(['MAILTO', 'non_person'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['ACCOUNT', 'MAILTO_cleaned', 'source', 'de_prop_given', 'l_name'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "dfo31.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['ACCOUNT', 'MAILTO_cleaned', 'de_prop_given', 'source', 'l_name'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "dfp1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine lookup with query dataset\n",
    "result = dfo31.append(dfp1, sort=False)\n",
    "result = result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               ACCOUNT          MAILTO_cleaned  source de_prop_given   l_name\n",
       "874134   1276880010027  carr kirlice v          owners                carr   \n",
       "924294   1301260010006  baker kenneth r mary l  owners                baker  \n",
       "1018582  490461         elizabeth cowper        prob    0             cowper \n",
       "1018519  490572         george g edwards        prob    0             edwards"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ACCOUNT</th>\n      <th>MAILTO_cleaned</th>\n      <th>source</th>\n      <th>de_prop_given</th>\n      <th>l_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>874134</th>\n      <td>1276880010027</td>\n      <td>carr kirlice v</td>\n      <td>owners</td>\n      <td></td>\n      <td>carr</td>\n    </tr>\n    <tr>\n      <th>924294</th>\n      <td>1301260010006</td>\n      <td>baker kenneth r mary l</td>\n      <td>owners</td>\n      <td></td>\n      <td>baker</td>\n    </tr>\n    <tr>\n      <th>1018582</th>\n      <td>490461</td>\n      <td>elizabeth cowper</td>\n      <td>prob</td>\n      <td>0</td>\n      <td>cowper</td>\n    </tr>\n    <tr>\n      <th>1018519</th>\n      <td>490572</td>\n      <td>george g edwards</td>\n      <td>prob</td>\n      <td>0</td>\n      <td>edwards</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "result.groupby('source').sample(n=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add two test rows to the df for future test. One for each group to verify the results\n",
    "# test_df = pd.DataFrame(columns=result.columns)\n",
    "# test_df.loc[0] = ['1111111111111', 'amir athari', 'athari', 'prob']\n",
    "# test_df.loc[1] = ['1111111111111', 'amir athari', 'athari', 'owners']\n",
    "# # Append all in one big df\n",
    "# result = result.append(test_df, sort=False)\n",
    "# result = result.reset_index(drop=True)\n",
    "# result = result.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick indexes of two groups for future slicing\n",
    "l_index = result[result['source']=='owners'].index\n",
    "q_index = result[result['source']=='prob'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS ASSUME the index is order need correction\n",
    "# # Lookup explicit index\n",
    "# lsi = result[result['source']=='owners'].index.values.astype(int)[0]\n",
    "# lei = result[result['source']=='owners'].index.values.astype(int)[-1]\n",
    "\n",
    "# # Query explici index\n",
    "# qsi = result[result['source']=='prob'].index.values.astype(int)[0]\n",
    "# qei = result[result['source']=='prob'].index.values.astype(int)[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_l = result[lsi:lei+1]\n",
    "# result_q = result[qsi:qei+1]\n",
    "# result_l.shape, result_q.shape"
   ]
  },
  {
   "source": [
    "# Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Here we use Tf-idf (term frequencyâ€“inverse document frequency) by looking at a normalized count where each word count is divided by the number of rows (i.e. document) this word appears in. I chose this method instead of for example bag-of-words becasue we are comparing names where on average every document has 3 words in it. And also I am not looking for similarities but actual exact match.\n",
    "\n",
    "I use Tf-idf twice, once to eliminate all documents from lookup dataset that do not have the exact last names like in query data. This will improve the performance. In a second round I will then compare the query data with a smaller loopup data using the second round of Tf-idf."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ACCOUNT, MAILTO_cleaned, source, de_prop_given, l_name]\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ACCOUNT</th>\n      <th>MAILTO_cleaned</th>\n      <th>source</th>\n      <th>de_prop_given</th>\n      <th>l_name</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "result[result['l_name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 1-1 word ngrams on last names only\n",
    "vectorizer = TfidfVectorizer(decode_error='replace', strip_accents='unicode', analyzer='word'\n",
    "                                       # ,stop_words='english'\n",
    "                                       ,ngram_range = (1, 1)\n",
    "                                       #, min_df = 1\n",
    "                                       , norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=True)#,\n",
    "                                       \n",
    "                                      #  max_df=1, max_features=None)\n",
    "X = vectorizer.fit_transform(result['l_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['00i', '1stop', '21st', '26th', '331eg', '396bp', '3c', '3lm', '3zca', '475xj', '4mb', '4mk', '4y', '6g', '88jg', 'a91', 'aaa', 'aad', 'aagaz', 'aah', 'aaker', 'aakerberg', 'aakquanakhann', 'aal', 'aalders', 'aaloori', 'aals', 'aalund', 'aaly', 'aamir', 'aamodt', 'aamoth', 'aana', 'aanderud', 'aanstoos', 'aaqid', 'aardsma', 'aaron', 'aaronfaridi', 'aarons', 'aaronson', 'aarup', 'aasen', 'aaseng', 'aaser', 'aaserud', 'aasgaard', 'aasim', 'aaz', 'aba']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1020102, 120327), 2, 1025585)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "#nd-array info\n",
    "X.shape, X.ndim, X.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1018347, 1755), 2, 1062335)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "# Filter away rows where there is no last name from query list \n",
    "# Get similarities of lookup and query dataset \n",
    "sim1 = X[l_index].dot(X[q_index].transpose())\n",
    "sim1.shape, sim1.ndim, sim1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(315440,)"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "# Get non zero values' indexes and their values\n",
    "nonzero_tup = np.stack(np.nonzero(sim1), axis=-1)\n",
    "# Filter away zeros and return a list of indexs where there was a match with last names only\n",
    "res_list1 = [x[0] for x in nonzero_tup]\n",
    "# Convert list to array as it is expensitve to remvoe duplicates in a large list\n",
    "res_array = np.array(res_list1)\n",
    "res_uniques = np.unique(res_array) # This is the smaller lookup dataset where there is one exact match of last name for query data\n",
    "res_uniques.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(315440, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "# Drop irrelevant rows from the original 'combined' dataset\n",
    "result1 = result.iloc[res_uniques]\n",
    "# Keep only \"owner\" rows\n",
    "result1 = result1[result['source']=='owners']\n",
    "result1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              ACCOUNT     MAILTO_cleaned  source de_prop_given    l_name\n",
       "125493  0651220080105  ramirez maritza p  owners                ramirez \n",
       "847090  1262310030043  martinez victor    owners                martinez"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ACCOUNT</th>\n      <th>MAILTO_cleaned</th>\n      <th>source</th>\n      <th>de_prop_given</th>\n      <th>l_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>125493</th>\n      <td>0651220080105</td>\n      <td>ramirez maritza p</td>\n      <td>owners</td>\n      <td></td>\n      <td>ramirez</td>\n    </tr>\n    <tr>\n      <th>847090</th>\n      <td>1262310030043</td>\n      <td>martinez victor</td>\n      <td>owners</td>\n      <td></td>\n      <td>martinez</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "result1.groupby('source').sample(n=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         ACCOUNT                    MAILTO_cleaned  source de_prop_given  \\\n",
       "0  0032180000023  garcia antonio                    owners                 \n",
       "1  0032180000024  garcia paul                       owners                 \n",
       "2  0032180000027  martinez carlo p graciela guzman  owners                 \n",
       "\n",
       "     l_name  \n",
       "0  garcia    \n",
       "1  garcia    \n",
       "2  martinez  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ACCOUNT</th>\n      <th>MAILTO_cleaned</th>\n      <th>source</th>\n      <th>de_prop_given</th>\n      <th>l_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0032180000023</td>\n      <td>garcia antonio</td>\n      <td>owners</td>\n      <td></td>\n      <td>garcia</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0032180000024</td>\n      <td>garcia paul</td>\n      <td>owners</td>\n      <td></td>\n      <td>garcia</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0032180000027</td>\n      <td>martinez carlo p graciela guzman</td>\n      <td>owners</td>\n      <td></td>\n      <td>martinez</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "# Prepare TF-IDf for a more focused lookup dataset\n",
    "# Append query data to the smaller lookup dataset\n",
    "result2 = result1.append(dfp1, sort=False)\n",
    "result2 = result2.reset_index(drop=True)\n",
    "result2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       ACCOUNT    MAILTO_cleaned source de_prop_given   l_name\n",
       "317192  489872  lea fradkin       prob   0             fradkin\n",
       "317193  489877  elizabeth hossan  prob   0             hossan \n",
       "317194  489879  clarissa metzger  prob   0             metzger"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ACCOUNT</th>\n      <th>MAILTO_cleaned</th>\n      <th>source</th>\n      <th>de_prop_given</th>\n      <th>l_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>317192</th>\n      <td>489872</td>\n      <td>lea fradkin</td>\n      <td>prob</td>\n      <td>0</td>\n      <td>fradkin</td>\n    </tr>\n    <tr>\n      <th>317193</th>\n      <td>489877</td>\n      <td>elizabeth hossan</td>\n      <td>prob</td>\n      <td>0</td>\n      <td>hossan</td>\n    </tr>\n    <tr>\n      <th>317194</th>\n      <td>489879</td>\n      <td>clarissa metzger</td>\n      <td>prob</td>\n      <td>0</td>\n      <td>metzger</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "result2.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(315440, 1755)"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "# Inspect the lookup and query part of the data prior to Tf-idf\n",
    "result2[result2['source']=='owners'].shape[0], result2[result2['source']=='prob'].shape[0], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize names using 1-4 n-grams\n",
    "vectorizer = TfidfVectorizer(decode_error='replace', strip_accents='unicode', analyzer='word'\n",
    "                                       # ,stop_words='english'\n",
    "                                       ,ngram_range = (1, 4)\n",
    "                                       #, min_df = 1\n",
    "                                       , norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=True)#,\n",
    "                                       \n",
    "                                      #  max_df=1, max_features=None)\n",
    "X1 = vectorizer.fit_transform(result2['MAILTO_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['11b', '11b mt', '11b mt sinai', '11b mt sinai ln', '1981', '1981 emily', '1981 emily knobloch', '1981 emily knobloch trst', '1st', '1st cont']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((317195, 492682), 2, 1536493)"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "#nd-array info\n",
    "X1.shape, X1.ndim, X1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1755, 492682), 315440)"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "# Get index of lookup and query features so can map to original df\n",
    "l_index = result2[result2['source']=='owners'].index\n",
    "q_index = result2[result2['source']=='prob'].index\n",
    "X1[q_index].shape, X1[l_index].shape[0]"
   ]
  },
  {
   "source": [
    "## Use of cosign similarities to find matching names"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "190.063227891922\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dot_prod_lst = []   # Collect the similarities\n",
    "l_i_lst = []        # Collect the lookup index\n",
    "q_i_lst = []        # Collect the query index \n",
    "tax_lst = []        # Collect the Account numbers from lookup\n",
    "dock_num_lst = []   # Collect the Account numbers from query\n",
    "l_name_lst = []     # Collect the index for names from lookup\n",
    "q_name_lst = []     # Collect the index for names from query\n",
    "q_target_lst = []   # Collect the index for targe labels\n",
    "for i in range(0,X1[q_index].shape[0]): # Loop the query list\n",
    "        dot=X1[l_index].dot(X1[q_index][i].transpose()) # Get dot product of each query row with all lookup\n",
    "        if dot.max() > 0.35:\n",
    "            g = np.round(dot.max(), 3)\n",
    "\n",
    "            ##### Below print out for inspection purposes\n",
    "            # print('Dot product {}.'.format(g))\n",
    "            # print('Lookup index i {}.'.format(i))\n",
    "            # print('----')\n",
    "            # print('Lookup source: {}.'.format(result2.iloc[l_index[i]][3]))\n",
    "            # print('Query source: {}.'.format(result2.iloc[q_index[i]][3]))\n",
    "            # print('----')\n",
    "            # print('Lookup Tax Num: {}.'.format(result2.iloc[l_index].iloc[dot.argmax()][0]))\n",
    "            # print('Lookup name: {}.'.format(result2.iloc[l_index].iloc[dot.argmax()][1]))\n",
    "            # print('Query name: {}.'.format(result2.iloc[q_index[i]][1]))\n",
    "            # print('Query Docket Num: {}.'.format(result2.iloc[q_index[i]][0]))\n",
    "            # print('________________')\n",
    "            ######\n",
    "            dot_prod_lst.append(g)\n",
    "            l_i_lst.append(i)\n",
    "            tax_lst.append(result2.iloc[l_index].iloc[dot.argmax()][0])\n",
    "            l_name_lst.append(result2.iloc[l_index].iloc[dot.argmax()][1])\n",
    "            q_name_lst.append(result2.iloc[l_index].iloc[dot.argmax()][3])\n",
    "            dock_num_lst.append(result2.iloc[q_index[i]][0])\n",
    "            q_name_lst.append(result2.iloc[q_index[i]][1])\n",
    "            q_target_lst.append(result2.iloc[q_index[i]][3])\n",
    "            \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['ACCOUNT', 'MAILTO_cleaned', 'source', 'de_prop_given', 'l_name'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "result2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "length of each collected list: 1322\nlength of each collected list: 0\nlength of each collected list: 1322\nlength of each collected list: 1322\nlength of each collected list: 1322\nlength of each collected list: 2644\nlength of each collected list: 1322\n"
     ]
    }
   ],
   "source": [
    "# Collected lists\n",
    "lst_of_lists = [l_i_lst, q_i_lst, tax_lst, dock_num_lst, l_name_lst, q_name_lst, q_target_lst]\n",
    "for i in lst_of_lists:\n",
    "    print(f'length of each collected list: {len(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------\n",
      "lookup for column 0: 0975020000033\n",
      "query columns 0: 489910\n",
      "---------\n",
      "lookup for column 1: metzger clarissa m % clarissa metzger sur trus\n",
      "query columns 1: gladys godkin\n",
      "---------\n",
      "lookup for column 2: owners\n",
      "query columns 2: prob\n",
      "---------\n",
      "lookup for column 3: \n",
      "query columns 3: 0\n",
      "---------\n",
      "lookup for column 4: metzger\n",
      "query columns 4: britton\n"
     ]
    }
   ],
   "source": [
    "# Helper loop to find the position of headers for building the df\n",
    "for i in range(0,result.shape[1]):\n",
    "    print('---------')\n",
    "    print(f'lookup for column {i}: {result2.iloc[l_index].iloc[dot.argmax()][i]}')\n",
    "    print(f'query columns {i}: {result2.iloc[q_index[i]][i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-fa9ced6fa9b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Prepare a df for all collected data from previous step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m sim_df = pd.DataFrame({'lookup_tax_id': tax_lst, 'lookup_name': l_name_lst \\\n\u001b[0m\u001b[0;32m      3\u001b[0m                      \u001b[1;33m,\u001b[0m \u001b[1;34m'query_dock_num'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdock_num_lst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'query_name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mq_name_lst\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                      \u001b[1;33m,\u001b[0m \u001b[1;34m'sim_score'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdot_prod_lst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lookup_index'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ml_i_lst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'target_labels'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mq_target_lst\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                      , }).sort_values(by=['sim_score'], ascending=False)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         ]\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "# Prepare a df for all collected data from previous step\n",
    "sim_df = pd.DataFrame({'lookup_tax_id': tax_lst, 'lookup_name': l_name_lst \\\n",
    "                     , 'query_dock_num': dock_num_lst, 'query_name': q_name_lst \\\n",
    "                     , 'sim_score': dot_prod_lst, 'lookup_index': l_i_lst, 'target_labels': q_target_lst \\\n",
    "                     , }).sort_values(by=['sim_score'], ascending=False)\n",
    "sim_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df with vectorized values, headings and original index\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names(), index=result.index)\n",
    "tfidf_df.reset_index()\n",
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame()\n",
    "for i in range(1,10):\n",
    "    d = cosine_similarity(tfidf_df, tfidf_df.iloc[i:i+1])\n",
    "    d.shape\n",
    "    flat_list = [item for sublist in d for item in sublist]\n",
    "    best_match_index = np.argmax(flat_list)\n",
    "    #print(best_match_index)\n",
    "    best_match_val = max(d)\n",
    "    if best_match_val.any() > .80:\n",
    "         print(result.iloc[best_match_index, 1])\n",
    "    \n",
    "#df_temp = cosine_similarity(tfidf_df, tfidf_df.iloc[1000:1001]) # Here I assume that the parent vector is stored as the first row in the dataframe, but you could also store it separately\n",
    "\n",
    "# n = 10 # or however many you want\n",
    "# n_largest = tfidf_df['dist'].nlargest(n )  # this contains the parent itself as the most similar entry, hence n+1 to get n children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT DELETE\n",
    "#https://stackoverflow.com/questions/53875473/cosine-similarity-for-very-large-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "owners_i = result[result['source']=='owners'].index\n",
    "hcad_df = result[result['source']=='hcad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get owners index\n",
    "owners_i = result[result['source']=='owners'].index\n",
    "# Create empty distance df\n",
    "dist_df = pd.DataFrame(index=result.index)#, columns=owners_i)\n",
    "# Add other meta data to dist df\n",
    "dist_df['ACCOUNT'] = result['ACCOUNT']\n",
    "dist_df['MAILTO_cleaned'] = result['MAILTO_cleaned']\n",
    "dist_df['source'] = result['source']\n",
    "# Move col to left\n",
    "cols = dist_df.columns.tolist()\n",
    "cols = cols[-3:] + cols[:-3]\n",
    "dist_df = dist_df[cols]\n",
    "dist_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(result2)\n",
    "similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove query rows\n",
    "result3 = result2[result2['source']=='hcad']\n",
    "# Remove extra dict values\n",
    "result4 = result3.drop(['source', 'MAILTO_cleaned'], axis=1)\n",
    "# Create dict\n",
    "hcad_d = result4.set_index('ACCOUNT').T.to_dict('list')\n",
    "# Show n first key:value\n",
    "# {k: hcad_d[k] for k in list(hcad_d)[:1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove query rows\n",
    "result10 = result2[result2['source']=='owners']\n",
    "# Remove extra dict values\n",
    "result11 = result10.drop(['source', 'MAILTO_cleaned'], axis=1)\n",
    "# Create dict\n",
    "owners_d = result11.set_index('ACCOUNT').T.to_dict('list')\n",
    "# Show n first key:value\n",
    "# {k:owners_d[k] for k in list(owners_d)[:1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_items = {k: x[k] for k in hcad_d if k in owners_d and hcad_d[k] == owners[k]}\n",
    "print (len(shared_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oa = np.array(list(owners_d.items())[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = dfp1['DOCK_NUM'].tolist()\n",
    "matching = [s for s in l if \"|\" in s]\n",
    "s = '||||'.join(l).lower()\n",
    "dock_num = [str(x) for x in s.split('||||') if x]\n",
    "################\n",
    "l = dfp1['DE_NAME'].tolist()\n",
    "matching = [s for s in l if \"|\" in s]\n",
    "s = '||||'.join(l).lower()\n",
    "de_name = [str(x) for x in s.split('||||') if x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[]\n",
    "# Part 01:\n",
    "for j,k in enumerate(tfidf_ddj.values):\n",
    "    for n in range(len(k)):\n",
    "        t.append([j,n,k[n]])\n",
    "\n",
    "# Part 02:\n",
    "qq=[]\n",
    "for i in range(len(t)):\n",
    "    if t[i][0]==t[i][1]:\n",
    "        qq.append([t[i][0],t[i][1],0])\n",
    "    else:\n",
    "        qq.append(t[i])\n",
    "qq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "u=defaultdict(list)\n",
    "\n",
    "# Part 01:\n",
    "for i in range(len(qq)):\n",
    "    u[qq[i][0]].append(qq[i][2])\n",
    "    \n",
    "updated_df=pd.DataFrame(u)\n",
    "\n",
    "# Part 02:\n",
    "position_maxVal=[]\n",
    "for i in range(len(updated_df)):\n",
    "    position_maxVal.append(np.argmax(updated_df[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_comp=[]\n",
    "\n",
    "for j in position_maxVal: # list of highest similarity index positions\n",
    "    # this creates in order our names w/ highest similiarity by row    \n",
    "    sent_comp.append(names[j])\n",
    "    sent_comp\n",
    "\n",
    "# names based on highest similarity value per row as DF\n",
    "similar_names=pd.DataFrame(sent_comp,columns=['Similar Names'])\n",
    "\n",
    "# similiarity values rounded 4 decimal places finding max value per row\n",
    "similarity_value_=pd.DataFrame(round(updated_df.max(axis=1),4), columns=['Similarity Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accounts and names\n",
    "p_accounts=pd.DataFrame(accounts,columns=['Accounts'])\n",
    "p_names=pd.DataFrame(names,columns=['Names'])\n",
    "p_dock_num=pd.DataFrame(dock_num,columns=['DockNum'])\n",
    "p_de_name=pd.DataFrame(de_name,columns=['De_Name'])\n",
    "\n",
    "# put everything together\n",
    "cos_sim_df=pd.concat([p_accounts, p_names,similar_names,similarity_value_, p_dock_num, p_de_name], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_df.head()"
   ]
  },
  {
   "source": [
    "## Work on first names"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fnames[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regext list of first names\n",
    "empty = []\n",
    "for count, value in enumerate(p_fnames):\n",
    "    empty.append(\"((?:\\s|^)\"+value+\"(?:\\s|$))\") # Regex get a whole word and not partial in any part\n",
    "\n",
    "reg_str = '|'.join(empty)\n",
    "fname_pat = re.compile(reg_str) #'^\\\\b'+reg_str+'\\\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_mask = dfo4[['MAILTO_cleaned']].apply(lambda x: x.str.contains(fname_pat, regex=True )).any(axis=1)\n",
    "dfo3.shape, dfo4.shape, dfo4[fn_mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo5 = dfo4[fn_mask].copy()\n",
    "dfo5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo5.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_path = r'D:\\dev_data\\re\\hcad'\n",
    "# export_file = import_path+\"\\\\\"+ \"hcad_w_pro_last_names\" + \".pkl\"\n",
    "# dfp0 = pd.read_csv(file1)\n",
    "### Save the temporary result as pickle to save time in future\n",
    "export_path = r'D:\\dev_data\\re\\hcad'\n",
    "path_to_file = export_path+\"\\\\\"+ \"hcad_w_pro_first_last_names\" + '.pkl'\n",
    "dfo5.to_pickle(path_to_file, protocol=4)"
   ]
  },
  {
   "source": [
    "## Import mddle processed work"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the marketing data\n",
    "import_path = r'D:\\dev_data\\re\\hcad'\n",
    "path_to_file = import_path+\"\\\\\"+'hcad_w_pro_first_last_names.pkl'\n",
    "dfo5 = pd.read_pickle(path_to_file)\n",
    "dfo5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo5.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo5.insert(0, 'batch_id', range(0, 0 + len(dfo5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp1 = dfp0[['DOCK_NUM', 'DE_NAME']]\n",
    "dfp1 = dfp1.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo6 = dfo5[dfo5['batch_id']<=2000-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo6.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo6 = dfo6.drop(['l_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo6.head(2)"
   ]
  },
  {
   "source": [
    "## Add names from query list"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp1 = dfp0[['DOCK_NUM', 'DE_NAME']]\n",
    "dfp1 = dfp1.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "dfp1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both lists\n",
    "dfo6 =dfo6.append(dfp1)\n",
    "dfo6.fillna('NaN', inplace=True)\n",
    "dfo6 = dfo6.applymap(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo6.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error='replace', strip_accents='unicode', analyzer='word',\n",
    "                                       stop_words='english', ngram_range=(1, 1), \n",
    "                                       norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=True)#,\n",
    "                                      # max_df=1, min_df=1, max_features=None)\n",
    "X = vectorizer.fit_transform(dfo6['MAILTO_cleaned'])\n",
    "print(vectorizer.get_feature_names()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df with vectorized values, headings and original index\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names(), index=dfo6.index)\n",
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "# Get pairwise similarities for the vectorized df\n",
    "tfidf_dfj = pd.DataFrame(cosine_similarity(tfidf_df, dense_output=True))\n",
    "t = time.time()-t1\n",
    "print(\"SELFTIMED:\", t)\n",
    "tfidf_dfj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dfj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = dfo6['MAILTO_cleaned'].tolist()\n",
    "matching = [s for s in l if \"|\" in s]\n",
    "s = '||||'.join(l).lower()\n",
    "sc = text_cleaner(s)\n",
    "names = [str(x) for x in sc.split('||||') if x]\n",
    "################\n",
    "l = dfo6['ACCOUNT'].tolist()\n",
    "matching = [s for s in l if \"|\" in s]\n",
    "s = '||||'.join(l).lower()\n",
    "accounts = [str(x) for x in s.split('||||') if x]\n",
    "################\n",
    "l = dfo6['DOCK_NUM'].tolist()\n",
    "matching = [s for s in l if \"|\" in s]\n",
    "s = '||||'.join(l).lower()\n",
    "dock_num = [str(x) for x in s.split('||||') if x]\n",
    "################\n",
    "l = dfo6['DE_NAME'].tolist()\n",
    "matching = [s for s in l if \"|\" in s]\n",
    "s = '||||'.join(l).lower()\n",
    "de_name = [str(x) for x in s.split('||||') if x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "t=[]\n",
    "\n",
    "# Part 01:\n",
    "for j,k in enumerate(tfidf_dfj.values):\n",
    "    for n in range(len(k)):\n",
    "        t.append([j,n,k[n]])\n",
    "\n",
    "# Part 02:\n",
    "qq=[]\n",
    "for i in range(len(t)):\n",
    "    if t[i][0]==t[i][1]:\n",
    "        qq.append([t[i][0],t[i][1],0])\n",
    "    else:\n",
    "        qq.append(t[i])\n",
    "qq[:5]\n",
    "\n",
    "t = time.time()-t1\n",
    "print(\"SELFTIMED:\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "u=defaultdict(list)\n",
    "\n",
    "# Part 01:\n",
    "for i in range(len(qq)):\n",
    "    u[qq[i][0]].append(qq[i][2])\n",
    "    \n",
    "updated_df=pd.DataFrame(u)\n",
    "\n",
    "# Part 02:\n",
    "position_maxVal=[]\n",
    "for i in range(len(updated_df)):\n",
    "    position_maxVal.append(np.argmax(updated_df[i]))\n",
    "\n",
    "t = time.time()-t1\n",
    "print(\"SELFTIMED:\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "sent_comp=[]\n",
    "\n",
    "for j in position_maxVal: # list of highest similarity index positions\n",
    "    # this creates in order our names w/ highest similiarity by row    \n",
    "    sent_comp.append(names[j])\n",
    "    sent_comp\n",
    "\n",
    "# names based on highest similarity value per row as DF\n",
    "similar_names=pd.DataFrame(sent_comp,columns=['Similar Names'])\n",
    "\n",
    "# similiarity values rounded 4 decimal places finding max value per row\n",
    "similarity_value_=pd.DataFrame(round(updated_df.max(axis=1),4), columns=['Similarity Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accounts and names\n",
    "p_names=pd.DataFrame(names,columns=['Names'])\n",
    "p_accounts=pd.DataFrame(accounts,columns=['Accounts'])\n",
    "p_dock_num=pd.DataFrame(dock_num,columns=['DockNum'])\n",
    "p_de_name=pd.DataFrame(de_name,columns=['De_Name'])\n",
    "\n",
    "# put everything together\n",
    "cos_sim_df=pd.concat([p_accounts, p_names,similar_names,similarity_value_, p_dock_num, p_de_name], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_df[~(cos_sim_df.DockNum.isnull()) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# DO NOT DELETE ############################################# \n",
    "\n",
    "# Help functions to gather basic descriptions\n",
    "def describe(df):\n",
    "    return pd.concat([df.describe().T,\n",
    "                      df.mad().rename('mean abs dev'),\n",
    "                      df.skew().rename('skew'),\n",
    "                      df.kurt().rename('kurt'),\n",
    "                      df.nunique().rename('unique')\n",
    "                     ], axis=1).T\n",
    "\n",
    "def data_type_summary(df):\n",
    "    # get Object data type summary\n",
    "    df_stat_object = pd.DataFrame([])\n",
    "    df_stat_object = df.describe(include = ['O'])\n",
    "    df_stat_object.loc['dtype'] = df.dtypes\n",
    "    df_stat_object.loc['size'] = len(df)\n",
    "    #df_stat_object.loc['% null'] = df.isnull().count().round(2)\n",
    "    #df_stat_object.loc['% null'] = ((df0.isnull().sum()/df0.shape[0])*100).round(3)\n",
    "    \n",
    "    \n",
    "    # get numerical data type summary\n",
    "    df_stat_num = pd.DataFrame([])\n",
    "    df_stat_num = df.describe(include = [np.number])\n",
    "    df_stat_num.loc['dtype'] = df.dtypes\n",
    "    df_stat_num.loc['size'] = len(df)\n",
    "    #df_stat_object.loc['% null'] = df.isnull().count().round(2)\n",
    "    df_stat_object.loc['% null'] = ((df.isnull().sum()/df0.shape[0])*100).round(3)\n",
    "    \n",
    "    # # get date data type summary\n",
    "    # df_stat_date = df.describe(include = ['datetime64']) \n",
    "    # df_stat_date.loc['dtype'] = df.dtypes\n",
    "    # df_stat_date.loc['size'] = len(df)\n",
    "    # #df_stat_object.loc['% null'] = df.isnull().count().round(2)\n",
    "    # #df_stat_object.loc['% null'] = ((df0.isnull().sum()/df0.shape[0])*100).round(3)\n",
    "    \n",
    "    result = pd.concat([df_stat_num, df_stat_object], axis=1, sort=False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# DO NOT DELETE ############################################# \n",
    "\n",
    "def describe_with_nulls(df):\n",
    "    nan_cols = [i for i in df.columns if df[i].isnull().any()]\n",
    "    # get Object data type summary\n",
    "    df_stat_num = pd.DataFrame([])\n",
    "    #df_stat_num = df0[nan_cols].describe(include = [np.number, 'O', 'datetime64']) \n",
    "    df_stat_num = df0[nan_cols].describe(include = [np.number, 'O']) \n",
    "    \n",
    "    df_stat_num.loc['dtype'] = df.dtypes\n",
    "    df_stat_num.loc['size'] = len(df)\n",
    "    #df_stat_num.loc['% null'] = df.isnull().count().round(2)\n",
    "    df_stat_num.loc['% null'] = ((df0.isnull().sum()/df0.shape[0])*100).round(3)\n",
    "    return df_stat_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# DO NOT DELETE ############################################# \n",
    "# Function to move specific column to the left side for easier view\n",
    "def move_to_left(df, column_name):\n",
    "    df= df[ [str(column_name)] + [ col for col in df.columns if col != str(column_name) ] ]\n",
    "    return df\n",
    "\n",
    "def move_to_left_get_dup(df, column_name):\n",
    "    df = df[ [str(column_name)] + [ col for col in df.columns if col != str(column_name) ] ]\n",
    "    df = df[df.duplicated(str(column_name))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset of data\n",
    "df1 = df0.sample(100000)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_with_nulls(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_cols = ['LEGAL_DSCR_1','LEGAL_DSCR_2','LEGAL_DSCR_3','LEGAL_DSCR_4']\n",
    "date_cols = ['NOTICE_DATE','LAST_INSPECTED_DATE','NEW_OWNER_DATE']\n",
    "num_cols = ['TOTAL_LAND_AREA', 'TOTAL_BUILDING_AREA', 'ACREAGE', 'LAND_VALUE',\\\n",
    "            'IMPROVEMENT_VALUE', 'ASSESSED_VALUE', 'NXT_BUILDING',\\\n",
    "            'TOTAL_APPRAISED_VALUE', 'TOTAL_MARKET_VALUE', 'PRIOR_LND_VALUE', 'YR_IMPR',\\\n",
    "            'PRIOR_IMPR_VALUE', 'PRIOR_TOTAL_APPRAISED_VALUE', 'PRIOR_TOTAL_MARKET_VALUE', 'TOTAL_RCN_VALUE']\n",
    "# lower case\n",
    "df1 = df1.apply(lambda x: x.astype(str).str.lower())\n",
    "df1[num_cols] = df1[num_cols].apply(pd.to_numeric, errors='coerce') # numeric type\n",
    "df1[num_cols] = df1[num_cols].fillna(0).astype(float) # important to fill Na before converting ot int\n",
    "df1['SITE_ADDR_3']= df1['SITE_ADDR_3'].astype(str).str.zfill(5) # fix the zip code\n",
    "df1['all_legal'] = df1[legal_cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1) # Combine all legal\n",
    "df1.drop(legal_cols, axis=1, inplace=True)\n",
    "df1['UNDELIVERABLE'].replace(\"n\", 0,inplace=True)\n",
    "df1['UNDELIVERABLE'].replace(\"y\", 1,inplace=True)\n",
    "df1['PROTESTED'].replace(\"n\", 0,inplace=True)\n",
    "df1['PROTESTED'].replace(\"y\", 1,inplace=True)\n",
    "\n",
    "# Calculate $/sqf\n",
    "\n",
    "df1['land_val_sqf'] = df1['LAND_VALUE']/df1['TOTAL_LAND_AREA']\n",
    "df1['build_val_sqf'] = df1['IMPROVEMENT_VALUE']/df1['TOTAL_BUILDING_AREA']\n",
    "df1['tot_appr_val_per_build_sqf'] = df1['TOTAL_APPRAISED_VALUE']/df1['TOTAL_BUILDING_AREA']\n",
    "df1['tot_mark_val_per_build_sqf'] = df1['TOTAL_MARKET_VALUE']/df1['TOTAL_BUILDING_AREA']\n",
    "\n",
    "# Filter away very low values\n",
    "df1 = df1[df1['TOTAL_APPRAISED_VALUE'] >10000 ]\n",
    "df1 = df1[df1['PRIOR_TOTAL_APPRAISED_VALUE'] >10000 ]\n",
    "df1 = df1[df1['LAND_VALUE'] >10000 ]\n",
    "\n",
    "# Add a unique column for concatination using when actual index cannot be used\n",
    "df1['unique_id'] = np.arange(df1.shape[0])\n",
    "move_to_left(df1, 'unique_id')\n",
    "df1['ones'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sum by groupby for each neigborhood\n",
    "df1['neighborhood_total'] = df1.groupby('NEIGHBORHOOD_CODE')['ones'].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_summary(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ownership age\n",
    "current_year = dt.datetime.today().year\n",
    "df1['NEW_OWNER_YEAR'] = pd.to_numeric(df1['NEW_OWNER_YEAR'], errors='coerce').astype('Int64')\n",
    "df1['new_owner_age']  = current_year - df1['NEW_OWNER_YEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tag as an absentee\n",
    "def is_absentee (x):\n",
    "    return (x['SITE_ADDR_1'][:5] != x['MAIL_ADDR_1'][:5])*1\n",
    "# Apply the  \n",
    "df1['SITE_ADDR_1'] = df1['SITE_ADDR_1'].astype(str)\n",
    "df1['MAIL_ADDR_1'] = df1['MAIL_ADDR_1'].astype(str)\n",
    "df1['absentee'] = df1.apply(is_absentee, axis=1)\n",
    "df1.drop(['SITE_ADDR_1','MAIL_ADDR_1'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict for state class categories divided into residential and commercial\n",
    "state_class_dict = {}\n",
    "state_class_residential = ['a1', 'a2', 'a3', 'b1', 'b2']\n",
    "state_class_commercial = set(df1['STATE_CLASS']) - set(state_class_residential)\n",
    "\n",
    "state_class_vals = ['residential', 'commercial']\n",
    "for key in set(df1['STATE_CLASS']):\n",
    "    for value in state_class_vals:\n",
    "        if key in state_class_residential:\n",
    "            state_class_dict[key] = 'residential'\n",
    "            #state_class_vals.remove(value) \n",
    "        else:\n",
    "            state_class_dict[key] = 'commercial'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_state_codes(row, state_codes_dict):\n",
    "#     return values_dict[row]\n",
    "def map_state_codes(row, values_dict):\n",
    "    return values_dict.get(row, 0)\n",
    "    \n",
    "#state_codes_dict = {'a1': 'residential', 'a2': 'residential','b1': 'residential', 'b2': 'residential','b1': 'residential'}\n",
    "df1['STATE_CLASS_TYPE'] = df1['STATE_CLASS'].apply(map_state_codes, args = (state_class_dict,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get object numbers before adding dummy \n",
    "df2_obj = df2.select_dtypes(include=['O'])\n",
    "df2_obj.shape, df2_obj.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical list to keep\n",
    "\n",
    "cat_lst = [\n",
    "       ##'MAIL_STATE', 'SITE_ADDR_2', 'SITE_ADDR_3', 'STATE_CLASS'\n",
    "       ##,'SCHOOL_DIST', 'MAP_FACET', \n",
    "       ## 'KEY_MAP'\n",
    "       'NEIGHBORHOOD_CODE'\n",
    "       ##,'MARKET_AREA_1', 'MARKET_AREA_2', 'NXT_BUILDING', 'NOTICE_DATE_YEAR', 'UNDELIVERABLE',\n",
    "       ##'LAST_INSPECTED_YEAR', 'LAST_INSPECTED_BY', 'NEW_OWNER_YEAR',\n",
    "       ##'NEW_OWNER_MONTH','STATE_CLASS_TYPE'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(['all_legal'], axis=1)\n",
    "\n",
    "# Get object numbers before adding dummy \n",
    "df2_obj = df2.select_dtypes(include=['O'])\n",
    "df2_obj.shape, df2_obj.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all count of all unique values of categorical data type\n",
    "pd.unique(df2[df2_obj.columns].values.ravel()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the scope of the dataset for a specific range of properties\n",
    "df2 = df2[df2[\"STATE_CLASS_TYPE\"]=='residential']\n",
    "df2 = df2[df2[\"NXT_BUILDING\"] ==1]\n",
    "df2 = df2[df2[\"TOTAL_BUILDING_AREA\"] <= 8000]\n",
    "df2 = df2[df2[\"TOTAL_BUILDING_AREA\"] > 0]\n",
    "df2 = df2[df2[\"TOTAL_LAND_AREA\"] <= 50000]\n",
    "df2 = df2[df2[\"neighborhood_total\"] >= 7]\n",
    "df2 = df2[df2[\"TOTAL_MARKET_VALUE\"] < 10000000]\n",
    "df2 = df2[df2[\"tot_appr_val_per_build_sqf\"] <=1000]\n",
    "df2 = df2[df2[\"tot_mark_val_per_build_sqf\"] <=1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['TOTAL_BUILDING_AREA', 'TOTAL_LAND_AREA', 'LAND_VALUE', \\\n",
    "        'IMPROVEMENT_VALUE', 'TOTAL_APPRAISED_VALUE', \\\n",
    "        'TOTAL_MARKET_VALUE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary analysis of owners who protested using pair-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "with sns.plotting_context(\"notebook\", font_scale=1.1):\n",
    "    sns.pairplot(df2 \\\n",
    "        , vars=col, hue='PROTESTED')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value per sqf for building vs. land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for plotting\n",
    "# separate x and y\n",
    "x = df2[\"TOTAL_BUILDING_AREA\"]\n",
    "y = df2[\"TOTAL_LAND_AREA\"]\n",
    "\n",
    "# instanciate the figure\n",
    "fig = plt.figure(figsize = (20, 6))\n",
    "# in this case we use gridspec.\n",
    "gs = fig.add_gridspec(5, 5)\n",
    "ax1 = fig.add_subplot(gs[:4, :-1])\n",
    "\n",
    "# plot the data\n",
    "# main axis: scatter plot\n",
    "ax1.scatter(x, y) #, c = df4.target.astype('category').cat.codes) \n",
    "\n",
    "# set the labels for x and y\n",
    "ax1.set_xlabel(\"Building Area, sqf\")\n",
    "ax1.set_ylabel(\"Land Area, sqf\")\n",
    "\n",
    "# set the title for the main plot\n",
    "ax1.set_title(\"Building vs Land Area, sqf\")\n",
    "\n",
    "# prettify the plot\n",
    "# get rid of some of the spines to make the plot nicer\n",
    "ax1.spines[\"right\"].set_color(\"None\")\n",
    "ax1.spines[\"top\"].set_color(\"None\")\n",
    "\n",
    "# using familiar slicing, get the bottom axes and plot\n",
    "ax2 = fig.add_subplot(gs[4:, :-1])\n",
    "ax2.hist(x, 40, orientation = 'vertical', color = \"pink\")\n",
    "\n",
    "# invert the axis (it looks up side down)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# prettify the plot\n",
    "# set the ticks to null\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "# no axis to make plot nicer\n",
    "ax2.axison = False\n",
    "\n",
    "# using familiar slicing, get the left axes and plot\n",
    "ax3 = fig.add_subplot(gs[:4, -1])\n",
    "ax3.hist(y, 40, orientation = \"horizontal\", color = \"pink\")\n",
    "\n",
    "# prettify the plot\n",
    "# set the ticks to null\n",
    "ax3.set_xticks([])\n",
    "ax3.set_yticks([])\n",
    "# no axis to make plot nicer\n",
    "ax3.axison = False\n",
    "\n",
    "# make all the figures look nicier\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focusing on areas with multiple properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_neighboorhood = df2[df2['neighborhood_total'] >= 30]\n",
    "df2_neighboorhood = df2_neighboorhood.sort_values(by='neighborhood_total', ascending=False)\n",
    "df2_neighboorhood.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 4), linewidth=5, facecolor='cyan')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "x = df2_neighboorhood['NEIGHBORHOOD_CODE']\n",
    "y = df2_neighboorhood['neighborhood_total']\n",
    "plt.xticks(rotation='vertical')\n",
    "ax.bar(x, y)\n",
    "ax.set(\n",
    "    xlim=(0, 100), \\\n",
    "    ylim=(50, 180), \\\n",
    "    xlabel='Neighborhood Code', ylabel='Frequency' \\\n",
    "    , title='Histogram')\n",
    "fig.suptitle('Neighborhood with multiple properties', size=20, y=1.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrepancies in land value within same neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df2_neighboorhood['NEIGHBORHOOD_CODE'], df2_neighboorhood['tot_appr_val_per_build_sqf']\n",
    "c, s = df2_neighboorhood['land_val_sqf'], df2_neighboorhood['neighborhood_total']\n",
    " \n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "scatter = ax.scatter(x, y, c=c \\\n",
    "    , s=s\n",
    "    )\n",
    "\n",
    "# produce a legend with the unique colors from the scatter\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                loc =\"lower center\", title=\"Mean Land $/sqf\", fontsize='small', ncol=7) \n",
    "ax.add_artist(legend1)\n",
    "\n",
    "# produce a legend with a cross section of sizes from the scatter\n",
    "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.6)\n",
    "legend2 = ax.legend(handles, labels, loc=\"upper right\", title=\"Size: No. Properties\")\n",
    "\n",
    "ax.set(xlim=(0, 100), ylim=(0, 180))\n",
    "plt.xlabel(r'Neighborhood Code')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel(r'% Mean Total Appraised Build $/sqf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrepancies in building value within same neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df2_neighboorhood['NEIGHBORHOOD_CODE'], df2_neighboorhood['land_val_sqf']\n",
    "c, s = df2_neighboorhood['tot_appr_val_per_build_sqf'], df2_neighboorhood['neighborhood_total']\n",
    " \n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "scatter = ax.scatter(x, y, c=c \\\n",
    "    , s=s\n",
    "    )\n",
    "\n",
    "# produce a legend with the unique colors from the scatter\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                loc =\"upper center\", title=\"Mean Build $/sqf\", fontsize='small', ncol=4) \n",
    "ax.add_artist(legend1)\n",
    "\n",
    "# produce a legend with a cross section of sizes from the scatter\n",
    "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.6)\n",
    "legend2 = ax.legend(handles, labels, loc=\"upper right\", title=\"Size: No. Properties\")\n",
    "ax.set(xlim=(0, 100), ylim=(0, 180))\n",
    "plt.xlabel(r'Neighborhood Code')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel(r'% Mean Total Land Appraised $/sqf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at central tendencies using groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(df2, index=['NEIGHBORHOOD_CODE'] \\\n",
    "    , values=['PROTESTED','TOTAL_MARKET_VALUE', 'TOTAL_APPRAISED_VALUE', 'PRIOR_TOTAL_APPRAISED_VALUE'\\\n",
    "    , 'TOTAL_BUILDING_AREA', 'TOTAL_LAND_AREA', 'LAND_VALUE' \\\n",
    "    ,'ones'], aggfunc=[np.sum,np.mean], fill_value=0)\n",
    "table = table.reset_index()\n",
    "table.columns = table.columns.map('_'.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.drop([\n",
    "    'sum_PRIOR_TOTAL_APPRAISED_VALUE', 'sum_TOTAL_APPRAISED_VALUE' \\\n",
    "    ,'sum_TOTAL_BUILDING_AREA', 'sum_TOTAL_MARKET_VALUE', 'mean_PROTESTED'\\\n",
    "    ,'sum_TOTAL_BUILDING_AREA','sum_TOTAL_LAND_AREA'\n",
    "    ,'mean_ones'], axis=1)\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some metrics\n",
    "table['percent_protested'] = ((table['sum_PROTESTED'] / table['sum_ones']) * 100).round()\n",
    "table['mean_market_excess_val'] = table['mean_TOTAL_MARKET_VALUE'] - table['mean_TOTAL_APPRAISED_VALUE']\n",
    "table['mean_appraised_1yr_delta_val'] = table['mean_TOTAL_APPRAISED_VALUE'] - table['mean_PRIOR_TOTAL_APPRAISED_VALUE']\n",
    "table['mean_appraised_per_sqf'] = table['mean_TOTAL_APPRAISED_VALUE'] / table['mean_TOTAL_BUILDING_AREA']\n",
    "table['mean_land_val_per_sqf'] = table['mean_TOTAL_LAND_AREA'] / table['mean_TOTAL_BUILDING_AREA']\n",
    "table = move_to_left(table, 'NEIGHBORHOOD_CODE_')\n",
    "table = table[table['sum_ones'] >=10] # Focus on neighborhood with multiple properties\n",
    "table = table.sort_values('mean_TOTAL_MARKET_VALUE', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = table['NEIGHBORHOOD_CODE_'], table['mean_TOTAL_MARKET_VALUE']\n",
    "c, s = table['mean_market_excess_val'], table['percent_protested']\n",
    " \n",
    "fig, ax = plt.subplots(figsize=(20, 4))\n",
    "scatter = ax.scatter(x, y, c=c \\\n",
    "    , s=s\n",
    "    )\n",
    "\n",
    "# produce a legend with the unique colors from the scatter\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                loc=\"upper center\", title=\"Mean Market Excess Value\", fontsize='small', ncol=3)               \n",
    "ax.add_artist(legend1)\n",
    "\n",
    "# produce a legend with a cross section of sizes from the scatter\n",
    "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.6)\n",
    "legend2 = ax.legend(handles, labels, loc=\"upper right\", title=\"No. Protesting \")\n",
    "ax.set(xlim=(0, 100), ylim=(10000, 1200000))\n",
    "plt.xlabel(r'Neighborhood Code')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel(r'% Mean Total Market Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_summary(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start working on Marketing Targe Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.shape, df_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p2 = df_p.copy()\n",
    "# Drop all exepct focus columns\n",
    "df_p2 = df_p2.loc[:, ['DOCK_NUM','DE_NAME', 'DE_FIRST_NAME', 'DE_MIDDLE_NAME', 'DE_LAST_NAME']]\n",
    "df_p2['f_l_name'] = df_p2['DE_FIRST_NAME'] + ' ' + df_p2['DE_LAST_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add dummy variables for some categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape, df2.shape, df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add categorical data of Neighborhood Code as binary\n",
    "df3 = pd.get_dummies(data= df3, columns=[\"NEIGHBORHOOD_CODE\"], prefix='neig_code')\n",
    "df3.columns = df3.columns.str.replace(\".\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = move_to_left(df2, 'NEIGHBORHOOD_CODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape, df2.shape, df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training, test and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The label data is only for to validate the unsupervised result and is not used in the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select size=size from subset of a\n",
    "np.random.seed(123)\n",
    "indices = np.random.choice(a=15000, size=3000, replace=False)\n",
    "\n",
    "# Y is the target variable\n",
    "numerics = ['int_', 'int8', 'uint8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "Y = df3['PROTESTED']\n",
    "X = df3[df3.columns.difference(['absentee'])]\n",
    "X_num = X.select_dtypes(include=numerics)\n",
    "X_col_names = X_num.columns\n",
    "X = X[X_col_names]\n",
    "Y = Y.iloc[indices]\n",
    "X = X.iloc[indices]\n",
    "\n",
    "# numerics = ['int_', 'int8', 'uint8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "# df7_num = df7.select_dtypes(include=numerics)\n",
    "#X = df3.drop(['UNDELIVERABLE'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 465)\n",
    "X_train, X_validation, y_train, y_validation   = train_test_split(X_train, y_train, test_size=0.25, random_state=465)\n",
    "\n",
    "train_index = range(0,len(X_train))\n",
    "validation_index = range(len(X_train),len(X_train)+len(X_validation))\n",
    "test_index = range(len(X_train)+len(X_validation), len(X_train)+len(X_validation)+len(X_test))\n",
    "\n",
    "print(\"df0.shape:{}, df1.shape:{}, df2.shape:{}, df3.shape:{}, X.shape:{}, Y.shape:{}, X_train.shape:{}, X_test.shape:{}, y_train.shape:{}, y_test.shape{}, y_val.shape{}\".format(df0.shape, df1.shape, df2.shape, df3.shape, X.shape, Y.shape, X_train.shape, X_test.shape, y_train.shape, y_test.shape, y_validation.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance MetricsSilhouette Score\n",
    "### Following we will use some of these metric to assess the clustering resutls:\n",
    "\n",
    "Silhouette Score \n",
    "Tis measure doesn't need to know the ground truth and can be used to check, at the same time, the intra-cluster cohesion and the inter-cluster separation. \n",
    "This value is bounded between -1 and 1. A value close to -1 indicates that the average intra-cluster distance is greater than the average nearest-cluster index.\n",
    "Viceversa, a value close to 1 indicates that the algorithm achieved a very good level of internal cohesion and inter-cluster separation.\n",
    "\n",
    "Completeness score\n",
    "This score is complementary to the Homogeneity score. Its purpose is to provide a piece of information about the assignment of samples belonging to the same class. \n",
    "Also, a good clustering algorithm should assign all samples with the same true label to the same cluster. \n",
    "\n",
    "Homogeneity score\n",
    "This score is useful to check whether the clustering algorithm meets the requirement that a cluster should contain only samples belonging to a single class. \n",
    "It's bounded between 0 and 1, with low values indicating a low homogeneity. \n",
    "\n",
    "V-score\n",
    "This score is the harmonic mean of precision and recall employed on clustering results when the ground truth is given.\n",
    "\n",
    "The Adjusted Mutual Info\n",
    "The main goal is of this score is to evaluate the level of agreement between Ytrue and Ypred without taking into account the permutations.\n",
    "This value is equal to 0 in the case of the total absence of agreement and equal to 1 when Ytrue and Ypred completely agree. \n",
    "\n",
    "Adjusted Rand Index\n",
    "This score is useful to compare the original label distribution with the clustering prediction. \n",
    "Ideally, we'd like to reproduce the exact ground truth distribution, but in general, this is very difficult in real-life scenarios. \n",
    "The RA measure is bounded between -1 and 1. A value close to -1 indicates a prevalence of wrong assignments, while a value close to 1 indicates that the clustering algorithm is correctly reproducing the ground truth distribution.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of PSA with Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the optimum components for the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "n_components = 2\n",
    "whiten = False\n",
    "random_state = 2020\n",
    "pca2 = PCA(n_components=n_components, random_state=random_state)\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "pca2 = PCA().fit(X_train_std)\n",
    "print('Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_start = time.time()\n",
    "# print('Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "ax.plot(pca2.explained_variance_)\n",
    "ax.set_xlabel('Components', fontsize=14)\n",
    "ax.set_ylabel('Explained Variance', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PCA with optimal n-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "n_components = 2\n",
    "whiten = False\n",
    "random_state = 2020\n",
    "pca = PCA(n_components=n_components, random_state=random_state)\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_train_std_PCA = pca.fit_transform(X_train_std)\n",
    "df_train_std_PCA = pd.DataFrame(X_train_std_PCA, index=X_train.index)# columns=['x', 'y'], \n",
    "dff = pd.concat([X_train, df_train_std_PCA], axis=1)\n",
    "\n",
    "print('Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Compute the optimum inertia for kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "init='k-means++'\n",
    "\n",
    "for i in range(2, 70):\n",
    "    km = KMeans(n_clusters=i, max_iter=1000, random_state=1000)\n",
    "    km.fit(X_train_std_PCA )\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "# Show the plot inertia vs. no. clusters\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "ax.plot(np.arange(2, 70, 1), inertias)\n",
    "ax.set_xlabel('Number of clusters', fontsize=14)\n",
    "ax.set_ylabel('Inertia', fontsize=14)\n",
    "ax.set_xticks(np.arange(2, 71, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any infinit or nan values may stop the fitting later\n",
    "np.isnan(X_train_std_PCA.any()), np.isfinite(X_train_std_PCA.all()), #np.isnan(dff.any()), np.isfinite(dff.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std_PCA.shape, X_train.shape, dff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time_start = time.time()\n",
    "n_clusters = [2, 20, 50, 80]\n",
    "mapping = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "\n",
    "km_inertia = []\n",
    "silhouette_sc = []\n",
    "# Show the silhouette plots\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for i, n in enumerate(n_clusters):\n",
    "    km = KMeans(n_clusters=n, random_state=1000)\n",
    "    y_pred = km.fit_predict(X_train_std_PCA)\n",
    "    df_km = pd.DataFrame(y_pred, columns=['prediction'], index=X_train.index)\n",
    "    dff = pd.concat([dff, df_km], axis=1)\n",
    "    km_inertia.append((n, km.inertia_))\n",
    "    silhouette_sc.append((n, silhouette_score(X_train_std_PCA, df_km['prediction'])))\n",
    "   \n",
    "    silhouette_values = silhouette_samples(X_train_std_PCA, df_km['prediction'])\n",
    "\n",
    "    ax[mapping[i]].set_xticks([-0.15, 0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "    ax[mapping[i]].set_yticks([])\n",
    "    ax[mapping[i]].set_title('%d clusters' % n)\n",
    "    ax[mapping[i]].set_xlim([-0.15, 1])\n",
    "    y_lower = 20\n",
    "\n",
    "    for t in range(n):\n",
    "        ct_values = silhouette_values[y_pred == t]\n",
    "        ct_values.sort()\n",
    "\n",
    "        y_upper = y_lower + ct_values.shape[0]\n",
    "\n",
    "        color = cm.Accent(float(t) / n)\n",
    "        ax[mapping[i]].fill_betweenx(np.arange(y_lower, y_upper), 0, ct_values, facecolor=color, edgecolor=color)\n",
    "\n",
    "        y_lower = y_upper + 20\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the performance of clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "# Plotting Elbow Curve\n",
    "x_iner = [x[0] for x in km_inertia]\n",
    "y_iner  = [x[1] for x in km_inertia]\n",
    "ax[0].plot(x_iner, y_iner)\n",
    "ax[0].set_xlabel('Number of Clusters')\n",
    "ax[0].set_ylabel('Intertia')\n",
    "ax[0].set_title('Elbow Curve')\n",
    "# Plotting Silhouetter Score\n",
    "x_sil = [x[0] for x in silhouette_sc]\n",
    "y_sil  = [x[1] for x in silhouette_sc]\n",
    "ax[1].plot(x_sil, y_sil)\n",
    "ax[1].set_xlabel('Number of Clusters')\n",
    "ax[1].set_ylabel('Silhouetter Score')\n",
    "ax[1].set_title('Silhouetter Score Curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick optimal number of clusters and calcualtes all performance metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA with optimal n-components\n",
    "time_start = time.time()\n",
    "km = KMeans(n_clusters=20, max_iter=1000, random_state=1000)\n",
    "y_pred = km.fit_predict(X_train_std_PCA)\n",
    "df_km = pd.DataFrame(y_pred, columns=['prediction'], index=X_train.index)\n",
    "kmdff = pd.concat([X_train, df_km], axis=1)\n",
    "print('Silhouette Score: {}'.format(silhouette_score(X_train_std_PCA, kmdff['prediction']).round(2)))\n",
    "print('Completeness: {}'.format(completeness_score(kmdff['PROTESTED'], kmdff['prediction']).round(2)))\n",
    "print('Homogeneity: {}'.format(homogeneity_score(kmdff['PROTESTED'], kmdff['prediction']).round(2)))\n",
    "print('V-Score: {}'.format(v_measure_score(kmdff['PROTESTED'], kmdff['prediction']).round(2)))\n",
    "print('Adj. Mutual info: {}'.format(adjusted_mutual_info_score(kmdff['PROTESTED'], kmdff['prediction']).round(2)))\n",
    "print('Adj. Rand score: {}'.format(adjusted_rand_score(kmdff['PROTESTED'], kmdff['prediction'])))\n",
    "\n",
    "print('Time elapsed: {} seconds'.format(time.time()-time_start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "Based on Silhouette score we can conclude that this clustering is not helpful. The score is closer to zero than one indicating the algorithm achieved a very low level of internal cohesion and inter-cluster separation.\n",
    "Similar conclusion is confirmed from other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN Custering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use t-SNE to reduce the dimentionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=15, random_state=1000)\n",
    "X_train_tsne = tsne.fit_transform(X_train_std)\n",
    "\n",
    "df_tsne = pd.DataFrame(X_train_tsne, columns=['x', 'y'], index=X_train.index) \n",
    "dff1 = pd.concat([X_train, df_tsne], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE visualization: Is there any relationship between value/sqf and the property tax being protested or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 11))\n",
    "\n",
    "with sns.plotting_context(\"notebook\", font_scale=1.5):\n",
    "    sns.scatterplot(x='x',\n",
    "    y='y',\n",
    "    size='build_val_sqf',\n",
    "    sizes=(30, 400),\n",
    "    style='PROTESTED',\n",
    "    palette=sns.color_palette(\"tab10\", 2),\n",
    "    data=dff1,\n",
    "    ax=ax)\n",
    "\n",
    "ax.set_xlabel(r'$x$', fontsize=14)\n",
    "ax.set_ylabel(r'$y$', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = DBSCAN(eps=15, min_samples=3, metric='minkowski', p=2)\n",
    "Y_pred_t = dst.fit_predict(X_train_tsne)\n",
    "silhouette_score(dff1, Y_pred_t, metric='minkowski', p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best epsilon \n",
    "eps_grid = np.linspace(0.3, 1.2, num=10) \n",
    "silhouette_scores = [] \n",
    "eps_best = eps_grid[0] \n",
    "silhouette_score_max = -1 \n",
    "model_best = None \n",
    "labels_best = None\n",
    "AR_Index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tsne_array = np.array(X_train_tsne)\n",
    "for eps in eps_grid: \n",
    "    # Train DBSCAN clustering model \n",
    "    model = DBSCAN(eps=eps, min_samples=5).fit(X_train_tsne_array) \n",
    " \n",
    "    # Extract labels \n",
    "    labels = model.labels_\n",
    "    \n",
    "    # Extract performance metric  \n",
    "    silhouette_score = round(metrics.silhouette_score(X_train_tsne_array, labels), 4) \n",
    "    silhouette_scores.append(silhouette_score) \n",
    "    print(\"Epsilon:\", eps, \" --> silhouette score:\", silhouette_score) \n",
    "    \n",
    "    # Store the best score and its associated epsilon value\n",
    "    if silhouette_score > silhouette_score_max: \n",
    "        silhouette_score_max = silhouette_score \n",
    "        eps_best = eps \n",
    "        model_best = model \n",
    "        labels_best = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot silhouette scores vs epsilon \n",
    "plt.figure() \n",
    "plt.bar(eps_grid, silhouette_scores, width=0.05, color='k', align='center') \n",
    "plt.title('Silhouette score vs epsilon') \n",
    " \n",
    "# Best params \n",
    "print(\"Best epsilon =\", eps_best) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the associated model and labels for best epsilon \n",
    "model = model_best  \n",
    "labels = labels_best\n",
    "\n",
    "# Check for unassigned datapoints in the labels \n",
    "offset = 0 \n",
    "if -1 in labels: \n",
    "    offset = 1 \n",
    "    \n",
    "# Number of clusters in the data  \n",
    "num_clusters = len(set(labels)) - offset  \n",
    " \n",
    "print(\"Estimated number of clusters =\", num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the core samples from the trained model \n",
    "mask_core = np.zeros(labels.shape, dtype=np.bool) \n",
    "mask_core[model.core_sample_indices_] = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot resultant clusters  \n",
    "from itertools import cycle \n",
    "plt.figure() \n",
    "labels_uniq = set(labels) \n",
    "markers = cycle('vo^s<>') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_label, marker in zip(labels_uniq, markers): \n",
    "    # Use black dots for unassigned datapoints \n",
    "    if cur_label == -1: \n",
    "        marker = '.' \n",
    " \n",
    "    # Create mask for the current label \n",
    "    cur_mask = (labels == cur_label) \n",
    " \n",
    "    cur_data = X_train_tsne_array[cur_mask & mask_core] \n",
    "    plt.scatter(cur_data[:, 0], cur_data[:, 1], marker=marker, \n",
    "             edgecolors='black', s=96, facecolors='none') \n",
    "    cur_data = X_train_tsne_array[cur_mask & ~mask_core] \n",
    "    plt.scatter(cur_data[:, 0], cur_data[:, 1], marker=marker, \n",
    "             edgecolors='black', s=32) \n",
    "\n",
    "plt.title('Data separated into clusters') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "Based on Silhouette score we can conclude that this clustering is not helpful. The score is closer to zero than one indicating the algorithm achieved a very low level of internal cohesion and inter-cluster separation. This is also confirmed Adjusted Rand Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clustered_dataset(X, Y):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "    ax.grid()\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "\n",
    "    markers = ['o', 'd', '^', 'x', '1', '2', '3', 's']\n",
    "    colors = ['r', 'b', 'g', 'c', 'm', 'k', 'y', '#cccfff']\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        ax.scatter(X[i, 0], X[i, 1], marker=markers[Y[i]], color=colors[Y[i]])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(1000)\n",
    "\n",
    "nb_samples = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the dataset\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "\n",
    "ax.scatter(X_train_std_PCA[:, 0], X_train_std_PCA[:, 1], marker='o', color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "# Compute the distance matrix\n",
    "Xdist = pdist(X_train_std_PCA, metric='euclidean')\n",
    "\n",
    "# Compute the linkage\n",
    "Xl = linkage(Xdist, method='ward')\n",
    "\n",
    "# Compute and show the dendrogram\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "Xd = dendrogram(Xl)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a proper number of clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=7, linkage='complete')\n",
    "Y_pred = ac.fit_predict(X_train_std_PCA)\n",
    "\n",
    "print('Silhouette score (Complete): %.3f' % silhouette_score(X_train_std_PCA, Y_pred))\n",
    "print('Adjusted Rand score (Complete): %.3f' % adjusted_rand_score(y_train, Y_pred))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "\n",
    "markers = ['o', 'd', '^', 'x', '1', '2', '3', 's']\n",
    "colors = ['r', 'b', 'g', 'c', 'm', 'k', 'y', '#cccfff']\n",
    "col_labels = list(set(ac.labels_))\n",
    "\n",
    "plt.scatter(X_train_std_PCA[:, 0], X_train_std_PCA[:, 1], c=ac.labels_)#, marker=markers[y_train[i]], color=colors[y_train[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "Based on Silhouette score we can conclude that this clustering is not helpful. However it is much better than previous algorithms. \n",
    "The Adjusted Rand Index is also very low indicating a poor comparability between the original label distribution and the clustering prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model selection\n",
    "Hierarchical clustering performed best compared with other algorithms given the Silhouette score. However Adjusted Rand Index is low for all three which make this clustering project in need of further fine tuining and possibly adding more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}